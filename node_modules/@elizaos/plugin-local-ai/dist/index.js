// src/index.ts
import fs5 from "fs";
import os3 from "os";
import path5 from "path";
import { Readable as Readable2 } from "stream";
import { ModelType, logger as logger8 } from "@elizaos/core";
import {
  LlamaChatSession,
  getLlama
} from "node-llama-cpp";

// src/environment.ts
import { logger } from "@elizaos/core";
import { z } from "zod";
var DEFAULT_SMALL_MODEL = "DeepHermes-3-Llama-3-3B-Preview-q4.gguf";
var DEFAULT_LARGE_MODEL = "DeepHermes-3-Llama-3-8B-q4.gguf";
var DEFAULT_EMBEDDING_MODEL = "bge-small-en-v1.5.Q4_K_M.gguf";
var configSchema = z.object({
  LOCAL_SMALL_MODEL: z.string().optional().default(DEFAULT_SMALL_MODEL),
  LOCAL_LARGE_MODEL: z.string().optional().default(DEFAULT_LARGE_MODEL),
  LOCAL_EMBEDDING_MODEL: z.string().optional().default(DEFAULT_EMBEDDING_MODEL),
  MODELS_DIR: z.string().optional(),
  // Path for the models directory
  CACHE_DIR: z.string().optional(),
  // Path for the cache directory
  LOCAL_EMBEDDING_DIMENSIONS: z.string().optional().default("384").transform((val) => parseInt(val, 10))
  // Transform to number
});
function validateConfig() {
  try {
    const configToParse = {
      // Read model filenames from environment variables or use undefined (so zod defaults apply)
      LOCAL_SMALL_MODEL: process.env.LOCAL_SMALL_MODEL,
      LOCAL_LARGE_MODEL: process.env.LOCAL_LARGE_MODEL,
      LOCAL_EMBEDDING_MODEL: process.env.LOCAL_EMBEDDING_MODEL,
      MODELS_DIR: process.env.MODELS_DIR,
      // Read models directory path from env
      CACHE_DIR: process.env.CACHE_DIR,
      // Read cache directory path from env
      LOCAL_EMBEDDING_DIMENSIONS: process.env.LOCAL_EMBEDDING_DIMENSIONS
      // Read embedding dimensions
    };
    logger.debug("Validating configuration for local AI plugin from env:", {
      LOCAL_SMALL_MODEL: configToParse.LOCAL_SMALL_MODEL,
      LOCAL_LARGE_MODEL: configToParse.LOCAL_LARGE_MODEL,
      LOCAL_EMBEDDING_MODEL: configToParse.LOCAL_EMBEDDING_MODEL,
      MODELS_DIR: configToParse.MODELS_DIR,
      CACHE_DIR: configToParse.CACHE_DIR,
      LOCAL_EMBEDDING_DIMENSIONS: configToParse.LOCAL_EMBEDDING_DIMENSIONS
    });
    const validatedConfig = configSchema.parse(configToParse);
    logger.info("Using local AI configuration:", validatedConfig);
    return validatedConfig;
  } catch (error) {
    if (error instanceof z.ZodError) {
      const errorMessages = error.errors.map((err) => `${err.path.join(".")}: ${err.message}`).join("\n");
      logger.error("Zod validation failed:", errorMessages);
      throw new Error(`Configuration validation failed:
${errorMessages}`);
    }
    logger.error("Configuration validation failed:", {
      error: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : void 0
    });
    throw error;
  }
}

// src/types.ts
var MODEL_SPECS = {
  small: {
    name: "DeepHermes-3-Llama-3-3B-Preview-q4.gguf",
    repo: "NousResearch/DeepHermes-3-Llama-3-3B-Preview-GGUF",
    size: "3B",
    quantization: "Q4_0",
    contextSize: 8192,
    tokenizer: {
      name: "NousResearch/DeepHermes-3-Llama-3-3B-Preview",
      type: "llama"
    }
  },
  medium: {
    name: "DeepHermes-3-Llama-3-8B-q4.gguf",
    repo: "NousResearch/DeepHermes-3-Llama-3-8B-Preview-GGUF",
    size: "8B",
    quantization: "Q4_0",
    contextSize: 8192,
    tokenizer: {
      name: "NousResearch/DeepHermes-3-Llama-3-8B-Preview",
      type: "llama"
    }
  },
  embedding: {
    name: "bge-small-en-v1.5.Q4_K_M.gguf",
    repo: "ChristianAzinn/bge-small-en-v1.5-gguf",
    size: "133 MB",
    quantization: "Q4_K_M",
    contextSize: 512,
    dimensions: 384,
    tokenizer: {
      name: "ChristianAzinn/bge-small-en-v1.5-gguf",
      type: "llama"
    }
  },
  vision: {
    name: "Florence-2-base-ft",
    repo: "onnx-community/Florence-2-base-ft",
    size: "0.23B",
    modelId: "onnx-community/Florence-2-base-ft",
    contextSize: 1024,
    maxTokens: 256,
    tasks: [
      "CAPTION",
      "DETAILED_CAPTION",
      "MORE_DETAILED_CAPTION",
      "CAPTION_TO_PHRASE_GROUNDING",
      "OD",
      "DENSE_REGION_CAPTION",
      "REGION_PROPOSAL",
      "OCR",
      "OCR_WITH_REGION"
    ]
  },
  visionvl: {
    name: "Qwen2.5-VL-3B-Instruct",
    repo: "Qwen/Qwen2.5-VL-3B-Instruct",
    size: "3B",
    modelId: "Qwen/Qwen2.5-VL-3B-Instruct",
    contextSize: 32768,
    maxTokens: 1024,
    tasks: [
      "CAPTION",
      "DETAILED_CAPTION",
      "IMAGE_UNDERSTANDING",
      "VISUAL_QUESTION_ANSWERING",
      "OCR",
      "VISUAL_LOCALIZATION",
      "REGION_ANALYSIS"
    ]
  },
  tts: {
    default: {
      modelId: "Xenova/speecht5_tts",
      defaultSampleRate: 16e3,
      // SpeechT5 default
      // Use the standard embedding URL
      defaultSpeakerEmbeddingUrl: "https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin"
    }
  }
};

// src/utils/downloadManager.ts
import fs from "fs";
import https from "https";
import path from "path";
import { logger as logger2 } from "@elizaos/core";
var DownloadManager = class _DownloadManager {
  static instance = null;
  cacheDir;
  modelsDir;
  // Track active downloads to prevent duplicates
  activeDownloads = /* @__PURE__ */ new Map();
  /**
   * Creates a new instance of CacheManager.
   *
   * @param {string} cacheDir - The directory path for caching data.
   * @param {string} modelsDir - The directory path for model files.
   */
  constructor(cacheDir, modelsDir) {
    this.cacheDir = cacheDir;
    this.modelsDir = modelsDir;
    this.ensureCacheDirectory();
    this.ensureModelsDirectory();
  }
  /**
   * Returns the singleton instance of the DownloadManager class.
   * If an instance does not already exist, it creates a new one using the provided cache directory and models directory.
   *
   * @param {string} cacheDir - The directory where downloaded files are stored.
   * @param {string} modelsDir - The directory where model files are stored.
   * @returns {DownloadManager} The singleton instance of the DownloadManager class.
   */
  static getInstance(cacheDir, modelsDir) {
    if (!_DownloadManager.instance) {
      _DownloadManager.instance = new _DownloadManager(cacheDir, modelsDir);
    }
    return _DownloadManager.instance;
  }
  /**
   * Ensure that the cache directory exists.
   */
  ensureCacheDirectory() {
    if (!fs.existsSync(this.cacheDir)) {
      fs.mkdirSync(this.cacheDir, { recursive: true });
      logger2.debug("Created cache directory");
    }
  }
  /**
   * Ensure that the models directory exists. If it does not exist, create it.
   */
  ensureModelsDirectory() {
    logger2.debug("Ensuring models directory exists:", this.modelsDir);
    if (!fs.existsSync(this.modelsDir)) {
      fs.mkdirSync(this.modelsDir, { recursive: true });
      logger2.debug("Created models directory");
    }
  }
  /**
   * Downloads a file from a given URL to a specified destination path asynchronously.
   *
   * @param {string} url - The URL from which to download the file.
   * @param {string} destPath - The destination path where the downloaded file will be saved.
   * @returns {Promise<void>} A Promise that resolves when the file download is completed successfully or rejects if an error occurs.
   */
  async downloadFileInternal(url, destPath) {
    return new Promise((resolve, reject) => {
      logger2.info(`Starting download to: ${destPath}`);
      const tempPath = `${destPath}.tmp`;
      if (fs.existsSync(tempPath)) {
        try {
          logger2.warn(`Removing existing temporary file: ${tempPath}`);
          fs.unlinkSync(tempPath);
        } catch (err) {
          logger2.error(
            `Failed to remove existing temporary file: ${err instanceof Error ? err.message : String(err)}`
          );
        }
      }
      const request = https.get(
        url,
        {
          headers: {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
          },
          timeout: 3e5
          // Increase timeout to 5 minutes
        },
        (response) => {
          if (response.statusCode === 301 || response.statusCode === 302) {
            const redirectUrl = response.headers.location;
            if (!redirectUrl) {
              reject(new Error("Redirect location not found"));
              return;
            }
            this.activeDownloads.delete(destPath);
            this.downloadFile(redirectUrl, destPath).then(resolve).catch(reject);
            return;
          }
          if (response.statusCode !== 200) {
            reject(new Error(`Failed to download: ${response.statusCode}`));
            return;
          }
          const totalSize = Number.parseInt(response.headers["content-length"] || "0", 10);
          let downloadedSize = 0;
          let lastLoggedPercent = 0;
          const barLength = 30;
          const fileName = path.basename(destPath);
          logger2.info(`Downloading ${fileName}: ${"\u25B1".repeat(barLength)} 0%`);
          const file = fs.createWriteStream(tempPath);
          response.on("data", (chunk) => {
            downloadedSize += chunk.length;
            const percent = Math.round(downloadedSize / totalSize * 100);
            if (percent >= lastLoggedPercent + 5) {
              const filledLength = Math.floor(downloadedSize / totalSize * barLength);
              const progressBar = "\u25B0".repeat(filledLength) + "\u25B1".repeat(barLength - filledLength);
              logger2.info(`Downloading ${fileName}: ${progressBar} ${percent}%`);
              lastLoggedPercent = percent;
            }
          });
          response.pipe(file);
          file.on("finish", () => {
            file.close(() => {
              try {
                const completedBar = "\u25B0".repeat(barLength);
                logger2.info(`Downloading ${fileName}: ${completedBar} 100%`);
                const destDir = path.dirname(destPath);
                if (!fs.existsSync(destDir)) {
                  fs.mkdirSync(destDir, { recursive: true });
                }
                if (!fs.existsSync(tempPath)) {
                  reject(new Error(`Temporary file ${tempPath} does not exist`));
                  return;
                }
                if (fs.existsSync(destPath)) {
                  try {
                    const backupPath = `${destPath}.bak`;
                    fs.renameSync(destPath, backupPath);
                    logger2.info(`Created backup of existing file: ${backupPath}`);
                    fs.renameSync(tempPath, destPath);
                    if (fs.existsSync(backupPath)) {
                      fs.unlinkSync(backupPath);
                      logger2.info(`Removed backup file after successful update: ${backupPath}`);
                    }
                  } catch (moveErr) {
                    logger2.error(
                      `Error replacing file: ${moveErr instanceof Error ? moveErr.message : String(moveErr)}`
                    );
                    const backupPath = `${destPath}.bak`;
                    if (fs.existsSync(backupPath)) {
                      try {
                        fs.renameSync(backupPath, destPath);
                        logger2.info(`Restored from backup after failed update: ${backupPath}`);
                      } catch (restoreErr) {
                        logger2.error(
                          `Failed to restore from backup: ${restoreErr instanceof Error ? restoreErr.message : String(restoreErr)}`
                        );
                      }
                    }
                    if (fs.existsSync(tempPath)) {
                      try {
                        fs.unlinkSync(tempPath);
                      } catch (unlinkErr) {
                        logger2.error(
                          `Failed to clean up temp file: ${unlinkErr instanceof Error ? unlinkErr.message : String(unlinkErr)}`
                        );
                      }
                    }
                    reject(moveErr);
                    return;
                  }
                } else {
                  fs.renameSync(tempPath, destPath);
                }
                logger2.success(`Download of ${fileName} completed successfully`);
                this.activeDownloads.delete(destPath);
                resolve();
              } catch (err) {
                logger2.error(
                  `Error finalizing download: ${err instanceof Error ? err.message : String(err)}`
                );
                if (fs.existsSync(tempPath)) {
                  try {
                    fs.unlinkSync(tempPath);
                  } catch (unlinkErr) {
                    logger2.error(
                      `Failed to clean up temp file: ${unlinkErr instanceof Error ? unlinkErr.message : String(unlinkErr)}`
                    );
                  }
                }
                this.activeDownloads.delete(destPath);
                reject(err);
              }
            });
          });
          file.on("error", (err) => {
            logger2.error(`File write error: ${err instanceof Error ? err.message : String(err)}`);
            file.close(() => {
              if (fs.existsSync(tempPath)) {
                try {
                  fs.unlinkSync(tempPath);
                } catch (unlinkErr) {
                  logger2.error(
                    `Failed to clean up temp file after error: ${unlinkErr instanceof Error ? unlinkErr.message : String(unlinkErr)}`
                  );
                }
              }
              this.activeDownloads.delete(destPath);
              reject(err);
            });
          });
        }
      );
      request.on("error", (err) => {
        logger2.error(`Request error: ${err instanceof Error ? err.message : String(err)}`);
        if (fs.existsSync(tempPath)) {
          try {
            fs.unlinkSync(tempPath);
          } catch (unlinkErr) {
            logger2.error(
              `Failed to clean up temp file after request error: ${unlinkErr instanceof Error ? unlinkErr.message : String(unlinkErr)}`
            );
          }
        }
        this.activeDownloads.delete(destPath);
        reject(err);
      });
      request.on("timeout", () => {
        logger2.error("Download timeout occurred");
        request.destroy();
        if (fs.existsSync(tempPath)) {
          try {
            fs.unlinkSync(tempPath);
          } catch (unlinkErr) {
            logger2.error(
              `Failed to clean up temp file after timeout: ${unlinkErr instanceof Error ? unlinkErr.message : String(unlinkErr)}`
            );
          }
        }
        this.activeDownloads.delete(destPath);
        reject(new Error("Download timeout"));
      });
    });
  }
  /**
   * Asynchronously downloads a file from the specified URL to the destination path.
   *
   * @param {string} url - The URL of the file to download.
   * @param {string} destPath - The destination path to save the downloaded file.
   * @returns {Promise<void>} A Promise that resolves once the file has been successfully downloaded.
   */
  async downloadFile(url, destPath) {
    if (this.activeDownloads.has(destPath)) {
      logger2.info(`Download for ${destPath} already in progress, waiting for it to complete...`);
      const existingDownload = this.activeDownloads.get(destPath);
      if (existingDownload) {
        return existingDownload;
      }
      logger2.warn(
        `Download for ${destPath} was marked as in progress but not found in tracking map`
      );
    }
    const downloadPromise = this.downloadFileInternal(url, destPath);
    this.activeDownloads.set(destPath, downloadPromise);
    try {
      return await downloadPromise;
    } catch (error) {
      this.activeDownloads.delete(destPath);
      throw error;
    }
  }
  /**
   * Downloads a model specified by the modelSpec and saves it to the provided modelPath.
   * If the model is successfully downloaded, returns true, otherwise returns false.
   *
   * @param {ModelSpec} modelSpec - The model specification containing repo and name.
   * @param {string} modelPath - The path where the model will be saved.
   * @returns {Promise<boolean>} - Indicates if the model was successfully downloaded or not.
   */
  async downloadModel(modelSpec, modelPath) {
    try {
      logger2.info("Starting local model download...");
      const modelDir = path.dirname(modelPath);
      if (!fs.existsSync(modelDir)) {
        logger2.info("Creating model directory:", modelDir);
        fs.mkdirSync(modelDir, { recursive: true });
      }
      if (!fs.existsSync(modelPath)) {
        const attempts = [
          {
            description: "LFS URL with GGUF suffix",
            url: `https://huggingface.co/${modelSpec.repo}/resolve/main/${modelSpec.name}?download=true`
          },
          {
            description: "LFS URL without GGUF suffix",
            url: `https://huggingface.co/${modelSpec.repo.replace("-GGUF", "")}/resolve/main/${modelSpec.name}?download=true`
          },
          {
            description: "Standard URL with GGUF suffix",
            url: `https://huggingface.co/${modelSpec.repo}/resolve/main/${modelSpec.name}`
          },
          {
            description: "Standard URL without GGUF suffix",
            url: `https://huggingface.co/${modelSpec.repo.replace("-GGUF", "")}/resolve/main/${modelSpec.name}`
          }
        ];
        let lastError = null;
        let downloadSuccess = false;
        for (const attempt of attempts) {
          try {
            logger2.info("Attempting model download:", {
              description: attempt.description,
              url: attempt.url,
              timestamp: (/* @__PURE__ */ new Date()).toISOString()
            });
            await this.downloadFile(attempt.url, modelPath);
            logger2.success(
              `Model download complete: ${modelSpec.name} using ${attempt.description}`
            );
            downloadSuccess = true;
            break;
          } catch (error) {
            lastError = error;
            logger2.warn("Model download attempt failed:", {
              description: attempt.description,
              error: error instanceof Error ? error.message : String(error),
              timestamp: (/* @__PURE__ */ new Date()).toISOString()
            });
          }
        }
        if (!downloadSuccess) {
          throw lastError || new Error("All download attempts failed");
        }
        return true;
      }
      logger2.info("Model already exists at:", modelPath);
      return false;
    } catch (error) {
      logger2.error("Model download failed:", {
        error: error instanceof Error ? error.message : String(error),
        modelPath,
        model: modelSpec.name
      });
      throw error;
    }
  }
  /**
   * Returns the cache directory path.
   *
   * @returns {string} The path of the cache directory.
   */
  getCacheDir() {
    return this.cacheDir;
  }
  /**
   * Downloads a file from a given URL to a specified destination path.
   *
   * @param {string} url - The URL of the file to download.
   * @param {string} destPath - The destination path where the file should be saved.
   * @returns {Promise<void>} A Promise that resolves once the file has been downloaded.
   */
  async downloadFromUrl(url, destPath) {
    return this.downloadFile(url, destPath);
  }
  /**
   * Ensures that the specified directory exists. If it does not exist, it will be created.
   * @param {string} dirPath - The path of the directory to ensure existence of.
   * @returns {void}
   */
  ensureDirectoryExists(dirPath) {
    if (!fs.existsSync(dirPath)) {
      fs.mkdirSync(dirPath, { recursive: true });
      logger2.info(`Created directory: ${dirPath}`);
    }
  }
};

// src/utils/platform.ts
import { exec } from "child_process";
import os from "os";
import { promisify } from "util";
import { logger as logger3 } from "@elizaos/core";
var execAsync = promisify(exec);
var PlatformManager = class _PlatformManager {
  static instance;
  capabilities = null;
  /**
   * Private constructor method.
   */
  constructor() {
  }
  /**
   * Get the singleton instance of the PlatformManager class
   * @returns {PlatformManager} The instance of PlatformManager
   */
  static getInstance() {
    if (!_PlatformManager.instance) {
      _PlatformManager.instance = new _PlatformManager();
    }
    return _PlatformManager.instance;
  }
  /**
   * Asynchronous method to initialize platform detection.
   *
   * @returns {Promise<void>} Promise that resolves once platform detection is completed.
   */
  async initialize() {
    try {
      logger3.info("Initializing platform detection...");
      this.capabilities = await this.detectSystemCapabilities();
    } catch (error) {
      logger3.error("Platform detection failed", { error });
      throw error;
    }
  }
  /**
   * Detects the system capabilities including platform, CPU information, GPU information,
   * supported backends, and recommended model size.
   *
   * @returns {Promise<SystemCapabilities>} Details of the system capabilities including platform, CPU info, GPU info,
   * recommended model size, and supported backends.
   */
  async detectSystemCapabilities() {
    const platform = process.platform;
    const cpuInfo = this.getCPUInfo();
    const gpu = await this.detectGPU();
    const supportedBackends = await this.getSupportedBackends(platform, gpu);
    const recommendedModelSize = this.getRecommendedModelSize(cpuInfo, gpu);
    return {
      platform,
      cpu: cpuInfo,
      gpu,
      recommendedModelSize,
      supportedBackends
    };
  }
  /**
   * Returns information about the CPU and memory of the system.
   * @returns {SystemCPU} The CPU information including model, number of cores, speed, architecture, and memory details.
   */
  getCPUInfo() {
    const cpus = os.cpus();
    const totalMemory = os.totalmem();
    const freeMemory = os.freemem();
    return {
      model: cpus[0].model,
      cores: cpus.length,
      speed: cpus[0].speed,
      architecture: process.arch,
      memory: {
        total: totalMemory,
        free: freeMemory
      }
    };
  }
  /**
   * Asynchronously detects the GPU information based on the current platform.
   * @returns A promise that resolves with the GPU information if detection is successful, otherwise null.
   */
  async detectGPU() {
    const platform = process.platform;
    try {
      switch (platform) {
        case "darwin":
          return await this.detectMacGPU();
        case "win32":
          return await this.detectWindowsGPU();
        case "linux":
          return await this.detectLinuxGPU();
        default:
          return null;
      }
    } catch (error) {
      logger3.error("GPU detection failed", { error });
      return null;
    }
  }
  /**
   * Asynchronously detects the GPU of a Mac system.
   * @returns {Promise<SystemGPU>} A promise that resolves to an object representing the detected GPU.
   */
  async detectMacGPU() {
    try {
      const { stdout } = await execAsync("sysctl -n machdep.cpu.brand_string");
      const isAppleSilicon = stdout.toLowerCase().includes("apple");
      if (isAppleSilicon) {
        return {
          name: "Apple Silicon",
          type: "metal",
          isAppleSilicon: true
        };
      }
      const { stdout: gpuInfo } = await execAsync("system_profiler SPDisplaysDataType");
      return {
        name: gpuInfo.split("Chipset Model:")[1]?.split("\n")[0]?.trim() || "Unknown GPU",
        type: "metal",
        isAppleSilicon: false
      };
    } catch (error) {
      logger3.error("Mac GPU detection failed", { error });
      return {
        name: "Unknown Mac GPU",
        type: "metal",
        isAppleSilicon: false
      };
    }
  }
  /**
   * Detects the GPU in a Windows system and returns information about it.
   *
   * @returns {Promise<SystemGPU | null>} A promise that resolves with the detected GPU information or null if detection fails.
   */
  async detectWindowsGPU() {
    try {
      const { stdout } = await execAsync("wmic path win32_VideoController get name");
      const gpuName = stdout.split("\n")[1].trim();
      if (gpuName.toLowerCase().includes("nvidia")) {
        const { stdout: nvidiaInfo } = await execAsync(
          "nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
        );
        const [name, memoryStr] = nvidiaInfo.split(",").map((s) => s.trim());
        const memory = Number.parseInt(memoryStr);
        return {
          name,
          memory,
          type: "cuda",
          version: await this.getNvidiaDriverVersion()
        };
      }
      return {
        name: gpuName,
        type: "directml"
      };
    } catch (error) {
      logger3.error("Windows GPU detection failed", { error });
      return null;
    }
  }
  /**
   * Asynchronously detects the GPU information for Linux systems.
   * Tries to detect NVIDIA GPU first using 'nvidia-smi' command and if successful,
   * returns the GPU name, memory size, type as 'cuda', and NVIDIA driver version.
   * If NVIDIA detection fails, it falls back to checking for other GPUs using 'lspci | grep -i vga' command.
   * If no GPU is detected, it returns null.
   *
   * @returns {Promise<SystemGPU | null>} The detected GPU information or null if detection fails.
   */
  async detectLinuxGPU() {
    try {
      const { stdout } = await execAsync(
        "nvidia-smi --query-gpu=name,memory.total --format=csv,noheader"
      );
      if (stdout) {
        const [name, memoryStr] = stdout.split(",").map((s) => s.trim());
        const memory = Number.parseInt(memoryStr);
        return {
          name,
          memory,
          type: "cuda",
          version: await this.getNvidiaDriverVersion()
        };
      }
    } catch {
      try {
        const { stdout } = await execAsync("lspci | grep -i vga");
        return {
          name: stdout.split(":").pop()?.trim() || "Unknown GPU",
          type: "none"
        };
      } catch (error) {
        logger3.error("Linux GPU detection failed", { error });
        return null;
      }
    }
    return null;
  }
  /**
   * Asynchronously retrieves the driver version of the Nvidia GPU using the 'nvidia-smi' command.
   *
   * @returns A promise that resolves with the driver version as a string, or 'unknown' if an error occurs.
   */
  async getNvidiaDriverVersion() {
    try {
      const { stdout } = await execAsync(
        "nvidia-smi --query-gpu=driver_version --format=csv,noheader"
      );
      return stdout.trim();
    } catch {
      return "unknown";
    }
  }
  /**
   * Retrieves the supported backends based on the platform and GPU type.
   * @param {NodeJS.Platform} platform - The platform on which the code is running.
   * @param {SystemGPU | null} gpu - The GPU information, if available.
   * @returns {Promise<Array<"cuda" | "metal" | "directml" | "cpu">>} - An array of supported backends including 'cuda', 'metal', 'directml', and 'cpu'.
   */
  async getSupportedBackends(platform, gpu) {
    const backends = ["cpu"];
    if (gpu) {
      switch (platform) {
        case "darwin":
          backends.push("metal");
          break;
        case "win32":
          if (gpu.type === "cuda") {
            backends.push("cuda");
          }
          backends.push("directml");
          break;
        case "linux":
          if (gpu.type === "cuda") {
            backends.push("cuda");
          }
          break;
      }
    }
    return backends;
  }
  /**
   * Determines the recommended model size based on the system's CPU and GPU.
   * @param {SystemCPU} cpu - The system's CPU.
   * @param {SystemGPU | null} gpu - The system's GPU, if available.
   * @returns {"small" | "medium" | "large"} - The recommended model size ("small", "medium", or "large").
   */
  getRecommendedModelSize(cpu, gpu) {
    if (gpu?.isAppleSilicon) {
      return cpu.memory.total > 16 * 1024 * 1024 * 1024 ? "medium" : "small";
    }
    if (gpu?.type === "cuda") {
      const gpuMemGB = (gpu.memory || 0) / 1024;
      if (gpuMemGB >= 16) return "large";
      if (gpuMemGB >= 8) return "medium";
    }
    if (cpu.memory.total > 32 * 1024 * 1024 * 1024) return "medium";
    return "small";
  }
  /**
   * Returns the SystemCapabilities of the PlatformManager.
   *
   * @returns {SystemCapabilities} The SystemCapabilities of the PlatformManager.
   * @throws {Error} if PlatformManager is not initialized.
   */
  getCapabilities() {
    if (!this.capabilities) {
      throw new Error("PlatformManager not initialized");
    }
    return this.capabilities;
  }
  /**
   * Checks if the device's GPU is Apple Silicon.
   * @returns {boolean} True if the GPU is Apple Silicon, false otherwise.
   */
  isAppleSilicon() {
    return !!this.capabilities?.gpu?.isAppleSilicon;
  }
  /**
   * Checks if the current device has GPU support.
   * @returns {boolean} - Returns true if the device has GPU support, false otherwise.
   */
  hasGPUSupport() {
    return !!this.capabilities?.gpu;
  }
  /**
   * Checks if the system supports CUDA GPU for processing.
   *
   * @returns {boolean} True if the system supports CUDA, false otherwise.
   */
  supportsCUDA() {
    return this.capabilities?.gpu?.type === "cuda";
  }
  /**
   * Check if the device supports Metal API for rendering graphics.
   * @returns {boolean} True if the device supports Metal, false otherwise.
   */
  supportsMetal() {
    return this.capabilities?.gpu?.type === "metal";
  }
  /**
   * Check if the device supports DirectML for GPU acceleration.
   *
   * @returns {boolean} True if the device supports DirectML, false otherwise.
   */
  supportsDirectML() {
    return this.capabilities?.gpu?.type === "directml";
  }
  /**
   * Get the recommended backend for computation based on the available capabilities.
   * @returns {"cuda" | "metal" | "directml" | "cpu"} The recommended backend for computation.
   * @throws {Error} Throws an error if PlatformManager is not initialized.
   */
  getRecommendedBackend() {
    if (!this.capabilities) {
      throw new Error("PlatformManager not initialized");
    }
    const { gpu, supportedBackends } = this.capabilities;
    if (gpu?.type === "cuda") return "cuda";
    if (gpu?.type === "metal") return "metal";
    if (supportedBackends.includes("directml")) return "directml";
    return "cpu";
  }
};
var getPlatformManager = () => {
  return PlatformManager.getInstance();
};

// src/utils/tokenizerManager.ts
import { logger as logger4 } from "@elizaos/core";
import { AutoTokenizer } from "@huggingface/transformers";
var TokenizerManager = class _TokenizerManager {
  static instance = null;
  tokenizers;
  cacheDir;
  modelsDir;
  /**
   * Constructor for creating a new instance of the class.
   *
   * @param {string} cacheDir - The directory for caching data.
   * @param {string} modelsDir - The directory for storing models.
   */
  constructor(cacheDir, modelsDir) {
    this.tokenizers = /* @__PURE__ */ new Map();
    this.cacheDir = cacheDir;
    this.modelsDir = modelsDir;
  }
  /**
   * Get the singleton instance of TokenizerManager class. If the instance does not exist, it will create a new one.
   *
   * @param {string} cacheDir - The directory to cache the tokenizer models.
   * @param {string} modelsDir - The directory where tokenizer models are stored.
   * @returns {TokenizerManager} The singleton instance of TokenizerManager.
   */
  static getInstance(cacheDir, modelsDir) {
    if (!_TokenizerManager.instance) {
      _TokenizerManager.instance = new _TokenizerManager(cacheDir, modelsDir);
    }
    return _TokenizerManager.instance;
  }
  /**
   * Asynchronously loads a tokenizer based on the provided ModelSpec configuration.
   *
   * @param {ModelSpec} modelConfig - The configuration object for the model to load the tokenizer for.
   * @returns {Promise<PreTrainedTokenizer>} - A promise that resolves to the loaded tokenizer.
   */
  async loadTokenizer(modelConfig) {
    try {
      const tokenizerKey = `${modelConfig.tokenizer.type}-${modelConfig.tokenizer.name}`;
      logger4.info("Loading tokenizer:", {
        key: tokenizerKey,
        name: modelConfig.tokenizer.name,
        type: modelConfig.tokenizer.type,
        modelsDir: this.modelsDir,
        cacheDir: this.cacheDir
      });
      if (this.tokenizers.has(tokenizerKey)) {
        logger4.info("Using cached tokenizer:", { key: tokenizerKey });
        const cachedTokenizer = this.tokenizers.get(tokenizerKey);
        if (!cachedTokenizer) {
          throw new Error(`Tokenizer ${tokenizerKey} exists in map but returned undefined`);
        }
        return cachedTokenizer;
      }
      const fs6 = await import("fs");
      if (!fs6.existsSync(this.modelsDir)) {
        logger4.warn("Models directory does not exist, creating it:", this.modelsDir);
        fs6.mkdirSync(this.modelsDir, { recursive: true });
      }
      logger4.info(
        "Initializing new tokenizer from HuggingFace with models directory:",
        this.modelsDir
      );
      try {
        const tokenizer = await AutoTokenizer.from_pretrained(modelConfig.tokenizer.name, {
          cache_dir: this.modelsDir,
          local_files_only: false
        });
        this.tokenizers.set(tokenizerKey, tokenizer);
        logger4.success("Tokenizer loaded successfully:", { key: tokenizerKey });
        return tokenizer;
      } catch (tokenizeError) {
        logger4.error("Failed to load tokenizer from HuggingFace:", {
          error: tokenizeError instanceof Error ? tokenizeError.message : String(tokenizeError),
          stack: tokenizeError instanceof Error ? tokenizeError.stack : void 0,
          tokenizer: modelConfig.tokenizer.name,
          modelsDir: this.modelsDir
        });
        logger4.info("Retrying tokenizer loading...");
        const tokenizer = await AutoTokenizer.from_pretrained(modelConfig.tokenizer.name, {
          cache_dir: this.modelsDir,
          local_files_only: false
        });
        this.tokenizers.set(tokenizerKey, tokenizer);
        logger4.success("Tokenizer loaded successfully on retry:", {
          key: tokenizerKey
        });
        return tokenizer;
      }
    } catch (error) {
      logger4.error("Failed to load tokenizer:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        model: modelConfig.name,
        tokenizer: modelConfig.tokenizer.name,
        modelsDir: this.modelsDir
      });
      throw error;
    }
  }
  /**
   * Encodes the given text using the specified tokenizer model configuration.
   *
   * @param {string} text - The text to encode.
   * @param {ModelSpec} modelConfig - The configuration for the model tokenizer.
   * @returns {Promise<number[]>} - An array of integers representing the encoded text.
   * @throws {Error} - If the text encoding fails, an error is thrown.
   */
  async encode(text, modelConfig) {
    try {
      logger4.info("Encoding text with tokenizer:", {
        length: text.length,
        tokenizer: modelConfig.tokenizer.name
      });
      const tokenizer = await this.loadTokenizer(modelConfig);
      logger4.info("Tokenizer loaded, encoding text...");
      const encoded = await tokenizer.encode(text, {
        add_special_tokens: true,
        return_token_type_ids: false
      });
      logger4.info("Text encoded successfully:", {
        tokenCount: encoded.length,
        tokenizer: modelConfig.tokenizer.name
      });
      return encoded;
    } catch (error) {
      logger4.error("Text encoding failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        textLength: text.length,
        tokenizer: modelConfig.tokenizer.name,
        modelsDir: this.modelsDir
      });
      throw error;
    }
  }
  /**
   * Asynchronously decodes an array of tokens using a tokenizer based on the provided ModelSpec.
   *
   * @param {number[]} tokens - The array of tokens to be decoded.
   * @param {ModelSpec} modelConfig - The ModelSpec object containing information about the model and tokenizer to be used.
   * @returns {Promise<string>} - A Promise that resolves with the decoded text.
   * @throws {Error} - If an error occurs during token decoding.
   */
  async decode(tokens, modelConfig) {
    try {
      logger4.info("Decoding tokens with tokenizer:", {
        count: tokens.length,
        tokenizer: modelConfig.tokenizer.name
      });
      const tokenizer = await this.loadTokenizer(modelConfig);
      logger4.info("Tokenizer loaded, decoding tokens...");
      const decoded = await tokenizer.decode(tokens, {
        skip_special_tokens: true,
        clean_up_tokenization_spaces: true
      });
      logger4.info("Tokens decoded successfully:", {
        textLength: decoded.length,
        tokenizer: modelConfig.tokenizer.name
      });
      return decoded;
    } catch (error) {
      logger4.error("Token decoding failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        tokenCount: tokens.length,
        tokenizer: modelConfig.tokenizer.name,
        modelsDir: this.modelsDir
      });
      throw error;
    }
  }
};

// src/utils/transcribeManager.ts
import { exec as exec2 } from "child_process";
import fs2 from "fs";
import path2 from "path";
import { promisify as promisify2 } from "util";
import { logger as logger5 } from "@elizaos/core";
var execAsync2 = promisify2(exec2);
var whisperModule = null;
async function getWhisper() {
  if (!whisperModule) {
    const module = await import("whisper-node");
    whisperModule = module.whisper;
  }
  return whisperModule;
}
var TranscribeManager = class _TranscribeManager {
  static instance = null;
  cacheDir;
  ffmpegAvailable = false;
  ffmpegVersion = null;
  ffmpegPath = null;
  ffmpegInitialized = false;
  /**
   * Constructor for TranscribeManager class.
   *
   * @param {string} cacheDir - The directory path for storing cached files.
   */
  constructor(cacheDir) {
    this.cacheDir = path2.join(cacheDir, "whisper");
    logger5.debug("Initializing TranscribeManager", {
      cacheDir: this.cacheDir,
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
    this.ensureCacheDirectory();
  }
  /**
   * Ensures that FFmpeg is initialized and available for use.
   * @returns {Promise<boolean>} A promise that resolves to a boolean value indicating if FFmpeg is available.
   */
  async ensureFFmpeg() {
    if (!this.ffmpegInitialized) {
      try {
        await this.initializeFFmpeg();
        this.ffmpegInitialized = true;
      } catch (error) {
        logger5.error("FFmpeg initialization failed:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
        return false;
      }
    }
    return this.ffmpegAvailable;
  }
  /**
   * Checks if FFmpeg is available.
   * @returns {boolean} True if FFmpeg is available, false otherwise.
   */
  isFFmpegAvailable() {
    return this.ffmpegAvailable;
  }
  /**
   * Asynchronously retrieves the FFmpeg version if it hasn't been fetched yet.
   * If the FFmpeg version has already been fetched, it will return the stored version.
   * @returns A Promise that resolves with the FFmpeg version as a string, or null if the version is not available.
   */
  async getFFmpegVersion() {
    if (!this.ffmpegVersion) {
      await this.fetchFFmpegVersion();
    }
    return this.ffmpegVersion;
  }
  /**
   * Fetches the FFmpeg version by executing the command "ffmpeg -version".
   * Updates the class property ffmpegVersion with the retrieved version.
   * Logs the FFmpeg version information or error message.
   * @returns {Promise<void>} A Promise that resolves once the FFmpeg version is fetched and logged.
   */
  async fetchFFmpegVersion() {
    try {
      const { stdout } = await execAsync2("ffmpeg -version");
      this.ffmpegVersion = stdout.split("\n")[0];
      logger5.info("FFmpeg version:", {
        version: this.ffmpegVersion,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
    } catch (error) {
      this.ffmpegVersion = null;
      logger5.error("Failed to get FFmpeg version:", {
        error: error instanceof Error ? error.message : String(error),
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
    }
  }
  /**
   * Initializes FFmpeg by performing the following steps:
   * 1. Checks for FFmpeg availability in PATH
   * 2. Retrieves FFmpeg version information
   * 3. Verifies FFmpeg capabilities
   *
   * If FFmpeg is available, logs a success message with version, path, and timestamp.
   * If FFmpeg is not available, logs installation instructions.
   *
   * @returns A Promise that resolves once FFmpeg has been successfully initialized
   */
  async initializeFFmpeg() {
    try {
      await this.checkFFmpegAvailability();
      if (this.ffmpegAvailable) {
        await this.fetchFFmpegVersion();
        await this.verifyFFmpegCapabilities();
        logger5.success("FFmpeg initialized successfully", {
          version: this.ffmpegVersion,
          path: this.ffmpegPath,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
      } else {
        this.logFFmpegInstallInstructions();
      }
    } catch (error) {
      this.ffmpegAvailable = false;
      logger5.error("FFmpeg initialization failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      this.logFFmpegInstallInstructions();
    }
  }
  /**
   * Asynchronously checks for the availability of FFmpeg in the system by executing a command to find the FFmpeg location.
   * Updates the class properties `ffmpegPath` and `ffmpegAvailable` accordingly.
   * Logs relevant information such as FFmpeg location and potential errors using the logger.
   *
   * @returns A Promise that resolves with no value upon completion.
   */
  async checkFFmpegAvailability() {
    try {
      const { stdout, stderr } = await execAsync2("which ffmpeg || where ffmpeg");
      this.ffmpegPath = stdout.trim();
      this.ffmpegAvailable = true;
      logger5.info("FFmpeg found at:", {
        path: this.ffmpegPath,
        stderr: stderr ? stderr.trim() : void 0,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
    } catch (error) {
      this.ffmpegAvailable = false;
      this.ffmpegPath = null;
      logger5.error("FFmpeg not found in PATH:", {
        error: error instanceof Error ? error.message : String(error),
        stderr: error instanceof Error && "stderr" in error ? error.stderr : void 0,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
    }
  }
  /**
   * Verifies the FFmpeg capabilities by checking if FFmpeg supports the required codecs and formats.
   *
   * @returns {Promise<void>} A Promise that resolves if FFmpeg has the required codecs, otherwise rejects with an error message.
   */
  async verifyFFmpegCapabilities() {
    try {
      const { stdout } = await execAsync2("ffmpeg -codecs");
      const hasRequiredCodecs = stdout.includes("pcm_s16le") && stdout.includes("wav");
      if (!hasRequiredCodecs) {
        throw new Error("FFmpeg installation missing required codecs (pcm_s16le, wav)");
      }
    } catch (error) {
      logger5.error("FFmpeg capabilities verification failed:", {
        error: error instanceof Error ? error.message : String(error),
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      throw error;
    }
  }
  /**
   * Logs instructions on how to install FFmpeg if it is not properly installed.
   */
  logFFmpegInstallInstructions() {
    logger5.warn("FFmpeg is required but not properly installed. Please install FFmpeg:", {
      instructions: {
        mac: "brew install ffmpeg",
        ubuntu: "sudo apt-get install ffmpeg",
        windows: "choco install ffmpeg",
        manual: "Download from https://ffmpeg.org/download.html"
      },
      requiredVersion: "4.0 or later",
      requiredCodecs: ["pcm_s16le", "wav"],
      timestamp: (/* @__PURE__ */ new Date()).toISOString()
    });
  }
  /**
   * Gets the singleton instance of TranscribeManager, creates a new instance if it doesn't exist.
   *
   * @param {string} cacheDir - The directory path for caching transcriptions.
   * @returns {TranscribeManager} The singleton instance of TranscribeManager.
   */
  static getInstance(cacheDir) {
    if (!_TranscribeManager.instance) {
      _TranscribeManager.instance = new _TranscribeManager(cacheDir);
    }
    return _TranscribeManager.instance;
  }
  /**
   * Ensures that the cache directory exists. If it doesn't exist,
   * creates the directory using fs.mkdirSync with recursive set to true.
   * @returns {void}
   */
  ensureCacheDirectory() {
    if (!fs2.existsSync(this.cacheDir)) {
      fs2.mkdirSync(this.cacheDir, { recursive: true });
    }
  }
  /**
   * Converts an audio file to WAV format using FFmpeg.
   *
   * @param {string} inputPath - The input path of the audio file to convert.
   * @param {string} outputPath - The output path where the converted WAV file will be saved.
   * @returns {Promise<void>} A Promise that resolves when the conversion is completed.
   * @throws {Error} If FFmpeg is not installed or not properly configured, or if the audio conversion fails.
   */
  async convertToWav(inputPath, outputPath) {
    if (!this.ffmpegAvailable) {
      throw new Error(
        "FFmpeg is not installed or not properly configured. Please install FFmpeg to use audio transcription."
      );
    }
    try {
      const { stderr } = await execAsync2(
        `ffmpeg -y -loglevel error -i "${inputPath}" -acodec pcm_s16le -ar 16000 -ac 1 "${outputPath}"`
      );
      if (stderr) {
        logger5.warn("FFmpeg conversion error:", {
          stderr,
          inputPath,
          outputPath,
          timestamp: (/* @__PURE__ */ new Date()).toISOString()
        });
      }
      if (!fs2.existsSync(outputPath)) {
        throw new Error("WAV file was not created successfully");
      }
    } catch (error) {
      logger5.error("Audio conversion failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        command: `ffmpeg -y -loglevel error -i "${inputPath}" -acodec pcm_s16le -ar 16000 -ac 1 "${outputPath}"`,
        ffmpegAvailable: this.ffmpegAvailable,
        ffmpegVersion: this.ffmpegVersion,
        ffmpegPath: this.ffmpegPath,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      throw new Error(
        `Failed to convert audio to WAV format: ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }
  /**
   * Asynchronously preprocesses the audio by converting the provided audio buffer into a WAV file.
   * If FFmpeg is not installed, an error is thrown.
   *
   * @param {Buffer} audioBuffer The audio buffer to preprocess
   * @returns {Promise<string>} The path to the preprocessed WAV file
   * @throws {Error} If FFmpeg is not installed or if audio preprocessing fails
   */
  async preprocessAudio(audioBuffer) {
    if (!this.ffmpegAvailable) {
      throw new Error("FFmpeg is not installed. Please install FFmpeg to use audio transcription.");
    }
    try {
      const isWav = audioBuffer.length > 4 && audioBuffer.toString("ascii", 0, 4) === "RIFF" && audioBuffer.length > 12 && audioBuffer.toString("ascii", 8, 12) === "WAVE";
      const extension = isWav ? ".wav" : "";
      const tempInputFile = path2.join(this.cacheDir, `temp_input_${Date.now()}${extension}`);
      const tempWavFile = path2.join(this.cacheDir, `temp_${Date.now()}.wav`);
      fs2.writeFileSync(tempInputFile, audioBuffer);
      if (isWav) {
        try {
          const { stdout } = await execAsync2(
            `ffprobe -v error -show_entries stream=sample_rate,channels,bits_per_raw_sample -of json "${tempInputFile}"`
          );
          const probeResult = JSON.parse(stdout);
          const stream = probeResult.streams?.[0];
          if (stream?.sample_rate === "16000" && stream?.channels === 1 && (stream?.bits_per_raw_sample === 16 || stream?.bits_per_raw_sample === void 0)) {
            fs2.renameSync(tempInputFile, tempWavFile);
            return tempWavFile;
          }
        } catch (probeError) {
          logger5.debug("FFprobe failed, continuing with conversion:", probeError);
        }
      }
      await this.convertToWav(tempInputFile, tempWavFile);
      if (fs2.existsSync(tempInputFile)) {
        fs2.unlinkSync(tempInputFile);
      }
      return tempWavFile;
    } catch (error) {
      logger5.error("Audio preprocessing failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        ffmpegAvailable: this.ffmpegAvailable,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      throw new Error(
        `Failed to preprocess audio: ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }
  /**
   * Transcribes the audio buffer to text using whisper.
   *
   * @param {Buffer} audioBuffer The audio buffer to transcribe.
   * @returns {Promise<TranscriptionResult>} A promise that resolves with the transcription result.
   * @throws {Error} If FFmpeg is not installed or properly configured.
   */
  async transcribe(audioBuffer) {
    await this.ensureFFmpeg();
    if (!this.ffmpegAvailable) {
      throw new Error(
        "FFmpeg is not installed or not properly configured. Please install FFmpeg to use audio transcription."
      );
    }
    try {
      const wavFile = await this.preprocessAudio(audioBuffer);
      logger5.info("Starting transcription with whisper...");
      let segments;
      try {
        const whisper = await getWhisper();
        segments = await whisper(wavFile, {
          modelName: "tiny",
          modelPath: path2.join(this.cacheDir, "models"),
          // Specify where to store models
          whisperOptions: {
            language: "en",
            word_timestamps: false
            // We don't need word-level timestamps
          }
        });
      } catch (whisperError) {
        const errorMessage = whisperError instanceof Error ? whisperError.message : String(whisperError);
        if (errorMessage.includes("not found") || errorMessage.includes("download")) {
          logger5.error("Whisper model not found. Please run: npx whisper-node download");
          throw new Error(
            "Whisper model not found. Please install it with: npx whisper-node download"
          );
        }
        logger5.error("Whisper transcription error:", whisperError);
        throw whisperError;
      }
      if (fs2.existsSync(wavFile)) {
        fs2.unlinkSync(wavFile);
        logger5.info("Temporary WAV file cleaned up");
      }
      if (!segments || !Array.isArray(segments)) {
        logger5.warn("Whisper returned no segments (likely silence or very short audio)");
        return { text: "" };
      }
      if (segments.length === 0) {
        logger5.warn("No speech detected in audio");
        return { text: "" };
      }
      const cleanText = segments.map((segment) => segment.speech?.trim() || "").filter((text) => text).join(" ");
      logger5.success("Transcription complete:", {
        textLength: cleanText.length,
        segmentCount: segments.length,
        timestamp: (/* @__PURE__ */ new Date()).toISOString()
      });
      return { text: cleanText };
    } catch (error) {
      logger5.error("Transcription failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        ffmpegAvailable: this.ffmpegAvailable
      });
      throw error;
    }
  }
};

// src/utils/ttsManager.ts
import { logger as logger6 } from "@elizaos/core";
import { pipeline } from "@huggingface/transformers";
import fs3 from "fs";
import path3 from "path";
import { fetch as fetch2 } from "undici";
import { PassThrough, Readable } from "stream";
function getWavHeader(audioLength, sampleRate, channelCount = 1, bitsPerSample = 16) {
  const wavHeader = Buffer.alloc(44);
  wavHeader.write("RIFF", 0);
  wavHeader.writeUInt32LE(36 + audioLength, 4);
  wavHeader.write("WAVE", 8);
  wavHeader.write("fmt ", 12);
  wavHeader.writeUInt32LE(16, 16);
  wavHeader.writeUInt16LE(1, 20);
  wavHeader.writeUInt16LE(channelCount, 22);
  wavHeader.writeUInt32LE(sampleRate, 24);
  wavHeader.writeUInt32LE(sampleRate * bitsPerSample * channelCount / 8, 28);
  wavHeader.writeUInt16LE(bitsPerSample * channelCount / 8, 32);
  wavHeader.writeUInt16LE(bitsPerSample, 34);
  wavHeader.write("data", 36);
  wavHeader.writeUInt32LE(audioLength, 40);
  return wavHeader;
}
function prependWavHeader(readable, audioLength, sampleRate, channelCount = 1, bitsPerSample = 16) {
  const wavHeader = getWavHeader(audioLength, sampleRate, channelCount, bitsPerSample);
  let pushedHeader = false;
  const passThrough = new PassThrough();
  readable.on("data", (data) => {
    if (!pushedHeader) {
      passThrough.push(wavHeader);
      pushedHeader = true;
    }
    passThrough.push(data);
  });
  readable.on("end", () => {
    passThrough.end();
  });
  return passThrough;
}
var TTSManager = class _TTSManager {
  static instance = null;
  cacheDir;
  synthesizer = null;
  defaultSpeakerEmbedding = null;
  initialized = false;
  initializingPromise = null;
  constructor(cacheDir) {
    this.cacheDir = path3.join(cacheDir, "tts");
    this.ensureCacheDirectory();
    logger6.debug("TTSManager using Transformers.js initialized");
  }
  static getInstance(cacheDir) {
    if (!_TTSManager.instance) {
      _TTSManager.instance = new _TTSManager(cacheDir);
    }
    return _TTSManager.instance;
  }
  ensureCacheDirectory() {
    if (!fs3.existsSync(this.cacheDir)) {
      fs3.mkdirSync(this.cacheDir, { recursive: true });
      logger6.debug("Created TTS cache directory:", this.cacheDir);
    }
  }
  async initialize() {
    if (this.initializingPromise) {
      logger6.debug("TTS initialization already in progress, awaiting existing promise.");
      return this.initializingPromise;
    }
    if (this.initialized) {
      logger6.debug("TTS already initialized.");
      return;
    }
    this.initializingPromise = (async () => {
      try {
        logger6.info("Initializing TTS with Transformers.js backend...");
        const ttsModelSpec = MODEL_SPECS.tts.default;
        if (!ttsModelSpec) {
          throw new Error("Default TTS model specification not found in MODEL_SPECS.");
        }
        const modelName = ttsModelSpec.modelId;
        const speakerEmbeddingUrl = ttsModelSpec.defaultSpeakerEmbeddingUrl;
        logger6.info(`Loading TTS pipeline for model: ${modelName}`);
        this.synthesizer = await pipeline("text-to-audio", modelName);
        logger6.success(`TTS pipeline loaded successfully for model: ${modelName}`);
        if (speakerEmbeddingUrl) {
          const embeddingFilename = path3.basename(new URL(speakerEmbeddingUrl).pathname);
          const embeddingPath = path3.join(this.cacheDir, embeddingFilename);
          if (fs3.existsSync(embeddingPath)) {
            logger6.info("Loading default speaker embedding from cache...");
            const buffer = fs3.readFileSync(embeddingPath);
            this.defaultSpeakerEmbedding = new Float32Array(
              buffer.buffer,
              buffer.byteOffset,
              buffer.length / Float32Array.BYTES_PER_ELEMENT
            );
            logger6.success("Default speaker embedding loaded from cache.");
          } else {
            logger6.info(`Downloading default speaker embedding from: ${speakerEmbeddingUrl}`);
            const response = await fetch2(speakerEmbeddingUrl);
            if (!response.ok) {
              throw new Error(`Failed to download speaker embedding: ${response.statusText}`);
            }
            const buffer = await response.arrayBuffer();
            this.defaultSpeakerEmbedding = new Float32Array(buffer);
            fs3.writeFileSync(embeddingPath, Buffer.from(buffer));
            logger6.success("Default speaker embedding downloaded and cached.");
          }
        } else {
          logger6.warn(
            `No default speaker embedding URL specified for model ${modelName}. Speaker control may be limited.`
          );
          this.defaultSpeakerEmbedding = null;
        }
        if (!this.synthesizer) {
          throw new Error("TTS initialization failed: Pipeline not loaded.");
        }
        logger6.success("TTS initialization complete (Transformers.js)");
        this.initialized = true;
      } catch (error) {
        logger6.error("TTS (Transformers.js) initialization failed:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0
        });
        this.initialized = false;
        this.synthesizer = null;
        this.defaultSpeakerEmbedding = null;
        throw error;
      } finally {
        this.initializingPromise = null;
        logger6.debug("TTS initializingPromise cleared after completion/failure.");
      }
    })();
    return this.initializingPromise;
  }
  /**
   * Asynchronously generates speech from a given text using the Transformers.js pipeline.
   * @param {string} text - The text to generate speech from.
   * @returns {Promise<Readable>} A promise that resolves to a Readable stream containing the generated WAV audio data.
   * @throws {Error} If the TTS model is not initialized or if generation fails.
   */
  async generateSpeech(text) {
    try {
      await this.initialize();
      if (!this.synthesizer) {
        throw new Error("TTS Manager not properly initialized.");
      }
      logger6.info("Starting speech generation with Transformers.js for text:", {
        text: text.substring(0, 50) + "..."
      });
      const output = await this.synthesizer(text, {
        // Pass embedding only if it was loaded
        ...this.defaultSpeakerEmbedding && {
          speaker_embeddings: this.defaultSpeakerEmbedding
        }
      });
      const audioFloat32 = output.audio;
      const samplingRate = output.sampling_rate;
      logger6.info("Raw audio data received from pipeline:", {
        samplingRate,
        length: audioFloat32.length
      });
      if (!audioFloat32 || audioFloat32.length === 0) {
        throw new Error("TTS pipeline generated empty audio output.");
      }
      const pcmData = new Int16Array(audioFloat32.length);
      for (let i = 0; i < audioFloat32.length; i++) {
        const s = Math.max(-1, Math.min(1, audioFloat32[i]));
        pcmData[i] = s < 0 ? s * 32768 : s * 32767;
      }
      const audioBuffer = Buffer.from(pcmData.buffer);
      logger6.info("Audio data converted to 16-bit PCM Buffer:", {
        byteLength: audioBuffer.length
      });
      const audioStream = prependWavHeader(
        Readable.from(audioBuffer),
        audioBuffer.length,
        // Pass buffer length in bytes
        samplingRate,
        1,
        // Number of channels (assuming mono)
        16
        // Bit depth
      );
      logger6.success("Speech generation complete (Transformers.js)");
      return audioStream;
    } catch (error) {
      logger6.error("Transformers.js speech generation failed:", {
        error: error instanceof Error ? error.message : String(error),
        text: text.substring(0, 50) + "...",
        stack: error instanceof Error ? error.stack : void 0
      });
      throw error;
    }
  }
};

// src/utils/visionManager.ts
import { existsSync } from "fs";
import fs4 from "fs";
import os2 from "os";
import path4 from "path";
import process2 from "process";
import { logger as logger7 } from "@elizaos/core";
import {
  AutoProcessor,
  AutoTokenizer as AutoTokenizer2,
  Florence2ForConditionalGeneration,
  RawImage,
  env
} from "@huggingface/transformers";
var VisionManager = class _VisionManager {
  static instance = null;
  model = null;
  processor = null;
  tokenizer = null;
  modelsDir;
  cacheDir;
  initialized = false;
  downloadManager;
  modelDownloaded = false;
  tokenizerDownloaded = false;
  processorDownloaded = false;
  platformConfig;
  modelComponents = [
    { name: "embed_tokens", type: "embeddings" },
    { name: "vision_encoder", type: "encoder" },
    { name: "decoder_model_merged", type: "decoder" },
    { name: "encoder_model", type: "encoder" }
  ];
  /**
   * Constructor for VisionManager class.
   *
   * @param {string} cacheDir - The directory path for caching vision models.
   */
  constructor(cacheDir) {
    this.modelsDir = path4.join(path4.dirname(cacheDir), "models", "vision");
    this.cacheDir = cacheDir;
    this.ensureModelsDirExists();
    this.downloadManager = DownloadManager.getInstance(this.cacheDir, this.modelsDir);
    this.platformConfig = this.getPlatformConfig();
    logger7.debug("VisionManager initialized");
  }
  /**
   * Retrieves the platform configuration based on the operating system and architecture.
   * @returns {PlatformConfig} The platform configuration object with device, dtype, and useOnnx properties.
   */
  getPlatformConfig() {
    const platform = os2.platform();
    const arch = os2.arch();
    let config = {
      device: "cpu",
      dtype: "fp32",
      useOnnx: true
    };
    if (platform === "darwin" && (arch === "arm64" || arch === "aarch64")) {
      config = {
        device: "gpu",
        dtype: "fp16",
        useOnnx: true
      };
    } else if (platform === "win32" || platform === "linux") {
      const hasCuda = process2.env.CUDA_VISIBLE_DEVICES !== void 0;
      if (hasCuda) {
        config = {
          device: "gpu",
          dtype: "fp16",
          useOnnx: true
        };
      }
    }
    return config;
  }
  /**
   * Ensures that the models directory exists. If it does not exist, it creates the directory.
   */
  ensureModelsDirExists() {
    if (!existsSync(this.modelsDir)) {
      logger7.debug(`Creating models directory at: ${this.modelsDir}`);
      fs4.mkdirSync(this.modelsDir, { recursive: true });
    }
  }
  /**
   * Returns the singleton instance of VisionManager.
   * If an instance does not already exist, a new instance is created with the specified cache directory.
   *
   * @param {string} cacheDir - The directory where cache files will be stored.
   *
   * @returns {VisionManager} The singleton instance of VisionManager.
   */
  static getInstance(cacheDir) {
    if (!_VisionManager.instance) {
      _VisionManager.instance = new _VisionManager(cacheDir);
    }
    return _VisionManager.instance;
  }
  /**
   * Check if the cache exists for the specified model or tokenizer or processor.
   * @param {string} modelId - The ID of the model.
   * @param {"model" | "tokenizer" | "processor"} type - The type of the cache ("model", "tokenizer", or "processor").
   * @returns {boolean} - Returns true if cache exists, otherwise returns false.
   */
  checkCacheExists(modelId, type) {
    const modelPath = path4.join(this.modelsDir, modelId.replace("/", "--"), type);
    if (existsSync(modelPath)) {
      logger7.info(`${type} found at: ${modelPath}`);
      return true;
    }
    return false;
  }
  /**
   * Configures the model components based on the platform and architecture.
   * Sets the default data type (dtype) for components based on platform capabilities.
   * Updates all component dtypes to match the default dtype.
   */
  configureModelComponents() {
    const platform = os2.platform();
    const arch = os2.arch();
    let defaultDtype = "fp32";
    if (platform === "darwin" && (arch === "arm64" || arch === "aarch64")) {
      defaultDtype = "fp16";
    } else if ((platform === "win32" || platform === "linux") && process2.env.CUDA_VISIBLE_DEVICES !== void 0) {
      defaultDtype = "fp16";
    }
    this.modelComponents = this.modelComponents.map((component) => ({
      ...component,
      dtype: defaultDtype
    }));
    logger7.info("Model components configured with dtype:", {
      platform,
      arch,
      defaultDtype,
      components: this.modelComponents.map((c) => `${c.name}: ${c.dtype}`)
    });
  }
  /**
   * Get the model configuration based on the input component name.
   * @param {string} componentName - The name of the component to retrieve the configuration for.
   * @returns {object} The model configuration object containing device, dtype, and cache_dir.
   */
  getModelConfig(componentName) {
    const component = this.modelComponents.find((c) => c.name === componentName);
    return {
      device: this.platformConfig.device,
      dtype: component?.dtype || "fp32",
      cache_dir: this.modelsDir
    };
  }
  /**
   * Asynchronous method to initialize the vision model by loading Florence2 model, vision tokenizer, and vision processor.
   *
   * @returns {Promise<void>} - Promise that resolves once the initialization process is completed.
   * @throws {Error} - If there is an error during the initialization process.
   */
  async initialize() {
    try {
      if (this.initialized) {
        logger7.info("Vision model already initialized, skipping initialization");
        return;
      }
      logger7.info("Starting vision model initialization...");
      const modelSpec = MODEL_SPECS.vision;
      logger7.info("Configuring environment for vision model...");
      env.allowLocalModels = true;
      env.allowRemoteModels = true;
      if (this.platformConfig.useOnnx) {
        env.backends.onnx.enabled = true;
        env.backends.onnx.logLevel = "info";
      }
      logger7.info("Loading Florence2 model...");
      try {
        let lastProgress = -1;
        const modelCached = this.checkCacheExists(modelSpec.modelId, "model");
        const model = await Florence2ForConditionalGeneration.from_pretrained(modelSpec.modelId, {
          device: "cpu",
          cache_dir: this.modelsDir,
          local_files_only: modelCached,
          revision: "main",
          progress_callback: (progressInfo) => {
            if (modelCached || this.modelDownloaded) return;
            const progress = "progress" in progressInfo ? Math.max(0, Math.min(1, progressInfo.progress)) : 0;
            const currentProgress = Math.round(progress * 100);
            if (currentProgress > lastProgress + 9 || currentProgress === 100) {
              lastProgress = currentProgress;
              const barLength = 30;
              const filledLength = Math.floor(currentProgress / 100 * barLength);
              const progressBar = "\u25B0".repeat(filledLength) + "\u25B1".repeat(barLength - filledLength);
              logger7.info(`Downloading vision model: ${progressBar} ${currentProgress}%`);
              if (currentProgress === 100) this.modelDownloaded = true;
            }
          }
        });
        this.model = model;
        logger7.success("Florence2 model loaded successfully");
      } catch (error) {
        logger7.error("Failed to load Florence2 model:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0,
          modelId: modelSpec.modelId
        });
        throw error;
      }
      logger7.info("Loading vision tokenizer...");
      try {
        const tokenizerCached = this.checkCacheExists(modelSpec.modelId, "tokenizer");
        let tokenizerProgress = -1;
        this.tokenizer = await AutoTokenizer2.from_pretrained(modelSpec.modelId, {
          cache_dir: this.modelsDir,
          local_files_only: tokenizerCached,
          progress_callback: (progressInfo) => {
            if (tokenizerCached || this.tokenizerDownloaded) return;
            const progress = "progress" in progressInfo ? Math.max(0, Math.min(1, progressInfo.progress)) : 0;
            const currentProgress = Math.round(progress * 100);
            if (currentProgress !== tokenizerProgress) {
              tokenizerProgress = currentProgress;
              const barLength = 30;
              const filledLength = Math.floor(currentProgress / 100 * barLength);
              const progressBar = "\u25B0".repeat(filledLength) + "\u25B1".repeat(barLength - filledLength);
              logger7.info(`Downloading vision tokenizer: ${progressBar} ${currentProgress}%`);
              if (currentProgress === 100) this.tokenizerDownloaded = true;
            }
          }
        });
        logger7.success("Vision tokenizer loaded successfully");
      } catch (error) {
        logger7.error("Failed to load tokenizer:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0,
          modelId: modelSpec.modelId
        });
        throw error;
      }
      logger7.info("Loading vision processor...");
      try {
        const processorCached = this.checkCacheExists(modelSpec.modelId, "processor");
        let processorProgress = -1;
        this.processor = await AutoProcessor.from_pretrained(modelSpec.modelId, {
          device: "cpu",
          cache_dir: this.modelsDir,
          local_files_only: processorCached,
          progress_callback: (progressInfo) => {
            if (processorCached || this.processorDownloaded) return;
            const progress = "progress" in progressInfo ? Math.max(0, Math.min(1, progressInfo.progress)) : 0;
            const currentProgress = Math.round(progress * 100);
            if (currentProgress !== processorProgress) {
              processorProgress = currentProgress;
              const barLength = 30;
              const filledLength = Math.floor(currentProgress / 100 * barLength);
              const progressBar = "\u25B0".repeat(filledLength) + "\u25B1".repeat(barLength - filledLength);
              logger7.info(`Downloading vision processor: ${progressBar} ${currentProgress}%`);
              if (currentProgress === 100) this.processorDownloaded = true;
            }
          }
        });
        logger7.success("Vision processor loaded successfully");
      } catch (error) {
        logger7.error("Failed to load vision processor:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0,
          modelId: modelSpec.modelId
        });
        throw error;
      }
      this.initialized = true;
      logger7.success("Vision model initialization complete");
    } catch (error) {
      logger7.error("Vision model initialization failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        modelsDir: this.modelsDir
      });
      throw error;
    }
  }
  /**
   * Fetches an image from a given URL and returns the image data as a Buffer along with its MIME type.
   *
   * @param {string} url - The URL of the image to fetch.
   * @returns {Promise<{ buffer: Buffer; mimeType: string }>} Object containing the image data as a Buffer and its MIME type.
   */
  async fetchImage(url) {
    try {
      logger7.info(`Fetching image from URL: ${url.slice(0, 100)}...`);
      if (url.startsWith("data:")) {
        logger7.info("Processing data URL...");
        const [header, base64Data] = url.split(",");
        const mimeType2 = header.split(";")[0].split(":")[1];
        const buffer2 = Buffer.from(base64Data, "base64");
        logger7.info("Data URL processed successfully");
        return { buffer: buffer2, mimeType: mimeType2 };
      }
      const response = await fetch(url);
      if (!response.ok) {
        throw new Error(`Failed to fetch image: ${response.statusText}`);
      }
      const buffer = Buffer.from(await response.arrayBuffer());
      const mimeType = response.headers.get("content-type") || "image/jpeg";
      logger7.info("Image fetched successfully:", {
        mimeType,
        bufferSize: buffer.length,
        status: response.status
      });
      return { buffer, mimeType };
    } catch (error) {
      logger7.error("Failed to fetch image:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        url
      });
      throw error;
    }
  }
  /**
   * Processes the image from the provided URL using the initialized vision model components.
   * @param {string} imageUrl - The URL of the image to process.
   * @returns {Promise<{ title: string; description: string }>} An object containing the title and description of the processed image.
   */
  async processImage(imageUrl) {
    try {
      logger7.info("Starting image processing...");
      if (!this.initialized) {
        logger7.info("Vision model not initialized, initializing now...");
        await this.initialize();
      }
      if (!this.model || !this.processor || !this.tokenizer) {
        throw new Error("Vision model components not properly initialized");
      }
      logger7.info("Fetching image...");
      const { buffer, mimeType } = await this.fetchImage(imageUrl);
      logger7.info("Creating image blob...");
      const blob = new Blob([buffer], { type: mimeType });
      logger7.info("Converting blob to RawImage...");
      const image = await RawImage.fromBlob(blob);
      logger7.info("Processing image with vision processor...");
      const visionInputs = await this.processor(image);
      logger7.info("Constructing prompts...");
      const prompts = this.processor.construct_prompts("<DETAILED_CAPTION>");
      logger7.info("Tokenizing prompts...");
      const textInputs = this.tokenizer(prompts);
      logger7.info("Generating image description...");
      const generatedIds = await this.model.generate({
        ...textInputs,
        ...visionInputs,
        max_new_tokens: MODEL_SPECS.vision.maxTokens
      });
      logger7.info("Decoding generated text...");
      const generatedText = this.tokenizer.batch_decode(generatedIds, {
        skip_special_tokens: false
      })[0];
      logger7.info("Post-processing generation...");
      const result = this.processor.post_process_generation(
        generatedText,
        "<DETAILED_CAPTION>",
        image.size
      );
      const detailedCaption = result["<DETAILED_CAPTION>"];
      const response = {
        title: `${detailedCaption.split(".")[0]}.`,
        description: detailedCaption
      };
      logger7.success("Image processing complete:", {
        titleLength: response.title.length,
        descriptionLength: response.description.length
      });
      return response;
    } catch (error) {
      logger7.error("Image processing failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        imageUrl,
        modelInitialized: this.initialized,
        hasModel: !!this.model,
        hasProcessor: !!this.processor,
        hasTokenizer: !!this.tokenizer
      });
      throw error;
    }
  }
};

// src/index.ts
import { basename } from "path";
var wordsToPunish = [
  " please",
  " feel",
  " free",
  "!",
  "\u2013",
  "\u2014",
  "?",
  ".",
  ",",
  "; ",
  " cosmos",
  " tapestry",
  " tapestries",
  " glitch",
  " matrix",
  " cyberspace",
  " troll",
  " questions",
  " topics",
  " discuss",
  " basically",
  " simulation",
  " simulate",
  " universe",
  " like",
  " debug",
  " debugging",
  " wild",
  " existential",
  " juicy",
  " circuits",
  " help",
  " ask",
  " happy",
  " just",
  " cosmic",
  " cool",
  " joke",
  " punchline",
  " fancy",
  " glad",
  " assist",
  " algorithm",
  " Indeed",
  " Furthermore",
  " However",
  " Notably",
  " Therefore"
];
var LocalAIManager = class _LocalAIManager {
  static instance = null;
  llama;
  smallModel;
  mediumModel;
  embeddingModel;
  embeddingContext;
  ctx;
  sequence;
  chatSession;
  modelPath;
  mediumModelPath;
  embeddingModelPath;
  cacheDir;
  tokenizerManager;
  downloadManager;
  visionManager;
  activeModelConfig;
  embeddingModelConfig;
  transcribeManager;
  ttsManager;
  config = null;
  // Store validated config
  // Initialization state flag
  smallModelInitialized = false;
  mediumModelInitialized = false;
  embeddingInitialized = false;
  visionInitialized = false;
  transcriptionInitialized = false;
  ttsInitialized = false;
  environmentInitialized = false;
  // Add flag for environment initialization
  // Initialization promises to prevent duplicate initialization
  smallModelInitializingPromise = null;
  mediumModelInitializingPromise = null;
  embeddingInitializingPromise = null;
  visionInitializingPromise = null;
  transcriptionInitializingPromise = null;
  ttsInitializingPromise = null;
  environmentInitializingPromise = null;
  // Add promise for environment
  modelsDir;
  /**
   * Private constructor function to initialize base managers and paths.
   * Model paths are set after environment initialization.
   */
  constructor() {
    this.config = validateConfig();
    this._setupCacheDir();
    this.activeModelConfig = MODEL_SPECS.small;
    this.embeddingModelConfig = MODEL_SPECS.embedding;
  }
  /**
   * Post-validation initialization steps that require config to be set.
   * Called after config validation in initializeEnvironment.
   */
  _postValidateInit() {
    this._setupModelsDir();
    this.downloadManager = DownloadManager.getInstance(this.cacheDir, this.modelsDir);
    this.tokenizerManager = TokenizerManager.getInstance(this.cacheDir, this.modelsDir);
    this.visionManager = VisionManager.getInstance(this.cacheDir);
    this.transcribeManager = TranscribeManager.getInstance(this.cacheDir);
    this.ttsManager = TTSManager.getInstance(this.cacheDir);
  }
  /**
   * Sets up the models directory, reading from config or environment variables,
   * and ensures the directory exists.
   */
  _setupModelsDir() {
    const modelsDirEnv = this.config?.MODELS_DIR?.trim() || process.env.MODELS_DIR?.trim();
    if (modelsDirEnv) {
      this.modelsDir = path5.resolve(modelsDirEnv);
      logger8.info("Using models directory from MODELS_DIR environment variable:", this.modelsDir);
    } else {
      this.modelsDir = path5.join(os3.homedir(), ".eliza", "models");
      logger8.info(
        "MODELS_DIR environment variable not set, using default models directory:",
        this.modelsDir
      );
    }
    if (!fs5.existsSync(this.modelsDir)) {
      fs5.mkdirSync(this.modelsDir, { recursive: true });
      logger8.debug("Ensured models directory exists (created):", this.modelsDir);
    } else {
      logger8.debug("Models directory already exists:", this.modelsDir);
    }
  }
  /**
   * Sets up the cache directory, reading from config or environment variables,
   * and ensures the directory exists.
   */
  _setupCacheDir() {
    const cacheDirEnv = this.config?.CACHE_DIR?.trim() || process.env.CACHE_DIR?.trim();
    if (cacheDirEnv) {
      this.cacheDir = path5.resolve(cacheDirEnv);
      logger8.info("Using cache directory from CACHE_DIR environment variable:", this.cacheDir);
    } else {
      const cacheDir = path5.join(os3.homedir(), ".eliza", "cache");
      if (!fs5.existsSync(cacheDir)) {
        fs5.mkdirSync(cacheDir, { recursive: true });
        logger8.debug("Ensuring cache directory exists (created):", cacheDir);
      }
      this.cacheDir = cacheDir;
      logger8.info(
        "CACHE_DIR environment variable not set, using default cache directory:",
        this.cacheDir
      );
    }
    if (!fs5.existsSync(this.cacheDir)) {
      fs5.mkdirSync(this.cacheDir, { recursive: true });
      logger8.debug("Ensured cache directory exists (created):", this.cacheDir);
    } else {
      logger8.debug("Cache directory already exists:", this.cacheDir);
    }
  }
  /**
   * Retrieves the singleton instance of LocalAIManager. If an instance does not already exist, a new one is created and returned.
   * @returns {LocalAIManager} The singleton instance of LocalAIManager
   */
  static getInstance() {
    if (!_LocalAIManager.instance) {
      _LocalAIManager.instance = new _LocalAIManager();
    }
    return _LocalAIManager.instance;
  }
  /**
   * Initializes the environment by validating the configuration and setting model paths.
   * Now public to be callable from plugin init and model handlers.
   *
   * @returns {Promise<void>} A Promise that resolves once the environment has been successfully initialized.
   */
  async initializeEnvironment() {
    if (this.environmentInitialized) return;
    if (this.environmentInitializingPromise) {
      await this.environmentInitializingPromise;
      return;
    }
    this.environmentInitializingPromise = (async () => {
      try {
        logger8.info("Initializing environment configuration...");
        this.config = await validateConfig();
        this._postValidateInit();
        this.modelPath = path5.join(this.modelsDir, this.config.LOCAL_SMALL_MODEL);
        this.mediumModelPath = path5.join(this.modelsDir, this.config.LOCAL_LARGE_MODEL);
        this.embeddingModelPath = path5.join(this.modelsDir, this.config.LOCAL_EMBEDDING_MODEL);
        logger8.info("Using small model path:", basename(this.modelPath));
        logger8.info("Using medium model path:", basename(this.mediumModelPath));
        logger8.info("Using embedding model path:", basename(this.embeddingModelPath));
        logger8.info("Environment configuration validated and model paths set");
        this.environmentInitialized = true;
        logger8.success("Environment initialization complete");
      } catch (error) {
        logger8.error("Environment validation failed:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : void 0
        });
        this.environmentInitializingPromise = null;
        throw error;
      }
    })();
    await this.environmentInitializingPromise;
  }
  /**
   * Downloads the model based on the modelPath provided.
   * Determines the model spec and path based on the model type.
   *
   * @param {ModelTypeName} modelType - The type of model to download
   * @param {ModelSpec} [customModelSpec] - Optional custom model spec to use instead of the default
   * @returns A Promise that resolves to a boolean indicating whether the model download was successful.
   */
  async downloadModel(modelType, customModelSpec) {
    let modelSpec;
    let modelPathToDownload;
    await this.initializeEnvironment();
    if (customModelSpec) {
      modelSpec = customModelSpec;
      modelPathToDownload = modelType === ModelType.TEXT_EMBEDDING ? this.embeddingModelPath : modelType === ModelType.TEXT_LARGE ? this.mediumModelPath : this.modelPath;
    } else if (modelType === ModelType.TEXT_EMBEDDING) {
      modelSpec = MODEL_SPECS.embedding;
      modelPathToDownload = this.embeddingModelPath;
    } else {
      modelSpec = modelType === ModelType.TEXT_LARGE ? MODEL_SPECS.medium : MODEL_SPECS.small;
      modelPathToDownload = modelType === ModelType.TEXT_LARGE ? this.mediumModelPath : this.modelPath;
    }
    try {
      return await this.downloadManager.downloadModel(modelSpec, modelPathToDownload);
    } catch (error) {
      logger8.error("Model download failed:", {
        error: error instanceof Error ? error.message : String(error),
        modelType,
        modelPath: modelPathToDownload
      });
      throw error;
    }
  }
  /**
   * Asynchronously checks the platform capabilities.
   *
   * @returns {Promise<void>} A promise that resolves once the platform capabilities have been checked.
   */
  async checkPlatformCapabilities() {
    try {
      const platformManager = getPlatformManager();
      await platformManager.initialize();
      const capabilities = platformManager.getCapabilities();
      logger8.info("Platform capabilities detected:", {
        platform: capabilities.platform,
        gpu: capabilities.gpu?.type || "none",
        recommendedModel: capabilities.recommendedModelSize,
        supportedBackends: capabilities.supportedBackends
      });
    } catch (error) {
      logger8.warn("Platform detection failed:", error);
    }
  }
  /**
   * Initializes the LocalAI Manager for a given model type.
   *
   * @param {ModelTypeName} modelType - The type of model to initialize (default: ModelType.TEXT_SMALL)
   * @returns {Promise<void>} A promise that resolves when initialization is complete or rejects if an error occurs
   */
  async initialize(modelType = ModelType.TEXT_SMALL) {
    await this.initializeEnvironment();
    if (modelType === ModelType.TEXT_LARGE) {
      await this.lazyInitMediumModel();
    } else {
      await this.lazyInitSmallModel();
    }
  }
  /**
   * Asynchronously initializes the embedding model.
   *
   * @returns {Promise<void>} A promise that resolves once the initialization is complete.
   */
  async initializeEmbedding() {
    try {
      await this.initializeEnvironment();
      logger8.info("Initializing embedding model...");
      logger8.info("Models directory:", this.modelsDir);
      if (!fs5.existsSync(this.modelsDir)) {
        logger8.warn("Models directory does not exist, creating it:", this.modelsDir);
        fs5.mkdirSync(this.modelsDir, { recursive: true });
      }
      await this.downloadModel(ModelType.TEXT_EMBEDDING);
      if (!this.llama) {
        this.llama = await getLlama();
      }
      if (!this.embeddingModel) {
        logger8.info("Loading embedding model:", this.embeddingModelPath);
        this.embeddingModel = await this.llama.loadModel({
          modelPath: this.embeddingModelPath,
          // Use the correct path
          gpuLayers: 0,
          // Embedding models are typically small enough to run on CPU
          vocabOnly: false
        });
        this.embeddingContext = await this.embeddingModel.createEmbeddingContext({
          contextSize: this.embeddingModelConfig.contextSize,
          batchSize: 512
        });
        logger8.success("Embedding model initialized successfully");
      }
    } catch (error) {
      logger8.error("Embedding initialization failed with details:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        modelsDir: this.modelsDir,
        embeddingModelPath: this.embeddingModelPath
        // Log the path being used
      });
      throw error;
    }
  }
  /**
   * Generate embeddings using the proper LlamaContext.getEmbedding method.
   */
  async generateEmbedding(text) {
    try {
      await this.lazyInitEmbedding();
      if (!this.embeddingModel || !this.embeddingContext) {
        throw new Error("Failed to initialize embedding model");
      }
      logger8.info("Generating embedding for text", { textLength: text.length });
      const embeddingResult = await this.embeddingContext.getEmbeddingFor(text);
      const mutableEmbedding = [...embeddingResult.vector];
      const normalizedEmbedding = this.normalizeEmbedding(mutableEmbedding);
      logger8.info("Embedding generation complete", {
        dimensions: normalizedEmbedding.length
      });
      return normalizedEmbedding;
    } catch (error) {
      logger8.error("Embedding generation failed:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0,
        textLength: text?.length ?? "text is null"
      });
      const zeroDimensions = this.config?.LOCAL_EMBEDDING_DIMENSIONS ? this.config.LOCAL_EMBEDDING_DIMENSIONS : this.embeddingModelConfig.dimensions;
      return new Array(zeroDimensions).fill(0);
    }
  }
  /**
   * Normalizes an embedding vector using L2 normalization
   *
   * @param {number[]} embedding - The embedding vector to normalize
   * @returns {number[]} - The normalized embedding vector
   */
  normalizeEmbedding(embedding) {
    const squareSum = embedding.reduce((sum, val) => sum + val * val, 0);
    const norm = Math.sqrt(squareSum);
    if (norm === 0) {
      return embedding;
    }
    return embedding.map((val) => val / norm);
  }
  /**
   * Lazy initialize the embedding model
   */
  async lazyInitEmbedding() {
    if (this.embeddingInitialized) return;
    if (!this.embeddingInitializingPromise) {
      this.embeddingInitializingPromise = (async () => {
        try {
          await this.initializeEnvironment();
          await this.downloadModel(ModelType.TEXT_EMBEDDING);
          if (!this.llama) {
            this.llama = await getLlama();
          }
          this.embeddingModel = await this.llama.loadModel({
            modelPath: this.embeddingModelPath,
            gpuLayers: 0,
            // Embedding models are typically small enough to run on CPU
            vocabOnly: false
          });
          this.embeddingContext = await this.embeddingModel.createEmbeddingContext({
            contextSize: this.embeddingModelConfig.contextSize,
            batchSize: 512
          });
          this.embeddingInitialized = true;
          logger8.info("Embedding model initialized successfully");
        } catch (error) {
          logger8.error("Failed to initialize embedding model:", error);
          this.embeddingInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.embeddingInitializingPromise;
  }
  /**
   * Asynchronously generates text based on the provided parameters.
   * Now uses lazy initialization for models
   */
  async generateText(params) {
    try {
      await this.initializeEnvironment();
      logger8.info("Generating text with model:", params.modelType);
      if (params.modelType === ModelType.TEXT_LARGE) {
        await this.lazyInitMediumModel();
        if (!this.mediumModel) {
          throw new Error("Medium model initialization failed");
        }
        this.activeModelConfig = MODEL_SPECS.medium;
        const mediumModel = this.mediumModel;
        this.ctx = await mediumModel.createContext({
          contextSize: MODEL_SPECS.medium.contextSize
        });
      } else {
        await this.lazyInitSmallModel();
        if (!this.smallModel) {
          throw new Error("Small model initialization failed");
        }
        this.activeModelConfig = MODEL_SPECS.small;
        const smallModel = this.smallModel;
        this.ctx = await smallModel.createContext({
          contextSize: MODEL_SPECS.small.contextSize
        });
      }
      if (!this.ctx) {
        throw new Error("Failed to create prompt");
      }
      this.sequence = this.ctx.getSequence();
      this.chatSession = new LlamaChatSession({
        contextSequence: this.sequence
      });
      if (!this.chatSession) {
        throw new Error("Failed to create chat session");
      }
      logger8.info("Created new chat session for model:", params.modelType);
      logger8.info("Incoming prompt structure:", {
        contextLength: params.prompt.length,
        hasAction: params.prompt.includes("action"),
        runtime: !!params.runtime,
        stopSequences: params.stopSequences
      });
      const tokens = await this.tokenizerManager.encode(params.prompt, this.activeModelConfig);
      logger8.info("Input tokens:", { count: tokens.length });
      const systemMessage = "You are a helpful AI assistant. Respond to the current request only.";
      await this.chatSession.prompt(systemMessage, {
        maxTokens: 1,
        // Minimal tokens for system message
        temperature: 0
      });
      let response = await this.chatSession.prompt(params.prompt, {
        maxTokens: 8192,
        temperature: 0.7,
        topP: 0.9,
        repeatPenalty: {
          punishTokensFilter: () => this.smallModel ? this.smallModel.tokenize(wordsToPunish.join(" ")) : [],
          penalty: 1.2,
          frequencyPenalty: 0.7,
          presencePenalty: 0.7
        }
      });
      logger8.info("Raw response structure:", {
        responseLength: response.length,
        hasAction: response.includes("action"),
        hasThinkTag: response.includes("<think>")
      });
      if (response.includes("<think>")) {
        logger8.info("Cleaning think tags from response");
        response = response.replace(/<think>[\s\S]*?<\/think>\n?/g, "");
        logger8.info("Think tags removed from response");
      }
      return response;
    } catch (error) {
      logger8.error("Text generation failed:", error);
      throw error;
    }
  }
  /**
   * Describe image with lazy vision model initialization
   */
  async describeImage(imageData, mimeType) {
    try {
      await this.lazyInitVision();
      const base64 = imageData.toString("base64");
      const dataUrl = `data:${mimeType};base64,${base64}`;
      return await this.visionManager.processImage(dataUrl);
    } catch (error) {
      logger8.error("Image description failed:", error);
      throw error;
    }
  }
  /**
   * Transcribe audio with lazy transcription model initialization
   */
  async transcribeAudio(audioBuffer) {
    try {
      await this.lazyInitTranscription();
      const result = await this.transcribeManager.transcribe(audioBuffer);
      return result.text;
    } catch (error) {
      logger8.error("Audio transcription failed:", {
        error: error instanceof Error ? error.message : String(error),
        bufferSize: audioBuffer.length
      });
      throw error;
    }
  }
  /**
   * Generate speech with lazy TTS model initialization
   */
  async generateSpeech(text) {
    try {
      await this.lazyInitTTS();
      return await this.ttsManager.generateSpeech(text);
    } catch (error) {
      logger8.error("Speech generation failed:", {
        error: error instanceof Error ? error.message : String(error),
        textLength: text.length
      });
      throw error;
    }
  }
  /**
   * Returns the TokenizerManager associated with this object.
   *
   * @returns {TokenizerManager} The TokenizerManager object.
   */
  getTokenizerManager() {
    return this.tokenizerManager;
  }
  /**
   * Returns the active model configuration.
   * @returns {ModelSpec} The active model configuration.
   */
  getActiveModelConfig() {
    return this.activeModelConfig;
  }
  /**
   * Lazy initialize the small text model
   */
  async lazyInitSmallModel() {
    if (this.smallModelInitialized) return;
    if (!this.smallModelInitializingPromise) {
      this.smallModelInitializingPromise = (async () => {
        await this.initializeEnvironment();
        await this.checkPlatformCapabilities();
        await this.downloadModel(ModelType.TEXT_SMALL);
        try {
          this.llama = await getLlama();
          const smallModel = await this.llama.loadModel({
            gpuLayers: 43,
            modelPath: this.modelPath,
            // Use the potentially overridden path
            vocabOnly: false
          });
          this.smallModel = smallModel;
          const ctx = await smallModel.createContext({
            contextSize: MODEL_SPECS.small.contextSize
          });
          this.ctx = ctx;
          this.sequence = void 0;
          this.smallModelInitialized = true;
          logger8.info("Small model initialized successfully");
        } catch (error) {
          logger8.error("Failed to initialize small model:", error);
          this.smallModelInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.smallModelInitializingPromise;
  }
  /**
   * Lazy initialize the medium text model
   */
  async lazyInitMediumModel() {
    if (this.mediumModelInitialized) return;
    if (!this.mediumModelInitializingPromise) {
      this.mediumModelInitializingPromise = (async () => {
        await this.initializeEnvironment();
        if (!this.llama) {
          await this.lazyInitSmallModel();
        }
        await this.downloadModel(ModelType.TEXT_LARGE);
        try {
          const mediumModel = await this.llama.loadModel({
            gpuLayers: 43,
            modelPath: this.mediumModelPath,
            // Use the potentially overridden path
            vocabOnly: false
          });
          this.mediumModel = mediumModel;
          this.mediumModelInitialized = true;
          logger8.info("Medium model initialized successfully");
        } catch (error) {
          logger8.error("Failed to initialize medium model:", error);
          this.mediumModelInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.mediumModelInitializingPromise;
  }
  /**
   * Lazy initialize the vision model
   */
  async lazyInitVision() {
    if (this.visionInitialized) return;
    if (!this.visionInitializingPromise) {
      this.visionInitializingPromise = (async () => {
        try {
          this.visionInitialized = true;
          logger8.info("Vision model initialized successfully");
        } catch (error) {
          logger8.error("Failed to initialize vision model:", error);
          this.visionInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.visionInitializingPromise;
  }
  /**
   * Lazy initialize the transcription model
   */
  async lazyInitTranscription() {
    if (this.transcriptionInitialized) return;
    if (!this.transcriptionInitializingPromise) {
      this.transcriptionInitializingPromise = (async () => {
        try {
          await this.initializeEnvironment();
          if (!this.transcribeManager) {
            this.transcribeManager = TranscribeManager.getInstance(this.cacheDir);
          }
          const ffmpegReady = await this.transcribeManager.ensureFFmpeg();
          if (!ffmpegReady) {
            logger8.error(
              "FFmpeg is not available or not configured correctly. Cannot proceed with transcription."
            );
            throw new Error(
              "FFmpeg is required for transcription but is not available. Please see server logs for installation instructions."
            );
          }
          this.transcriptionInitialized = true;
          logger8.info("Transcription prerequisites (FFmpeg) checked and ready.");
          logger8.info("Transcription model initialized successfully");
        } catch (error) {
          logger8.error("Failed to initialize transcription model:", error);
          this.transcriptionInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.transcriptionInitializingPromise;
  }
  /**
   * Lazy initialize the TTS model
   */
  async lazyInitTTS() {
    if (this.ttsInitialized) return;
    if (!this.ttsInitializingPromise) {
      this.ttsInitializingPromise = (async () => {
        try {
          await this.initializeEnvironment();
          this.ttsManager = TTSManager.getInstance(this.cacheDir);
          this.ttsInitialized = true;
          logger8.info("TTS model initialized successfully");
        } catch (error) {
          logger8.error("Failed to lazy initialize TTS components:", error);
          this.ttsInitializingPromise = null;
          throw error;
        }
      })();
    }
    await this.ttsInitializingPromise;
  }
};
var localAIManager = LocalAIManager.getInstance();
var localAiPlugin = {
  name: "local-ai",
  description: "Local AI plugin using LLaMA models",
  async init(_config, runtime) {
    logger8.info("\u{1F680} Initializing Local AI plugin...");
    try {
      await localAIManager.initializeEnvironment();
      const config = validateConfig();
      if (!config.LOCAL_SMALL_MODEL || !config.LOCAL_LARGE_MODEL || !config.LOCAL_EMBEDDING_MODEL) {
        logger8.warn("\u26A0\uFE0F Local AI plugin: Model configuration is incomplete");
        logger8.warn("Please ensure the following environment variables are set:");
        logger8.warn("- LOCAL_SMALL_MODEL: Path to small language model file");
        logger8.warn("- LOCAL_LARGE_MODEL: Path to large language model file");
        logger8.warn("- LOCAL_EMBEDDING_MODEL: Path to embedding model file");
        logger8.warn("Example: LOCAL_SMALL_MODEL=llama-3.2-1b-instruct-q8_0.gguf");
      }
      const modelsDir = config.MODELS_DIR || path5.join(os3.homedir(), ".eliza", "models");
      if (!fs5.existsSync(modelsDir)) {
        logger8.warn(`\u26A0\uFE0F Models directory does not exist: ${modelsDir}`);
        logger8.warn("The directory will be created, but you need to download model files");
        logger8.warn("Visit https://huggingface.co/models to download compatible GGUF models");
      }
      logger8.info("\u{1F50D} Testing Local AI initialization...");
      try {
        await localAIManager.checkPlatformCapabilities();
        const llamaInstance = await getLlama();
        if (llamaInstance) {
          logger8.success("\u2705 Local AI: llama.cpp library loaded successfully");
        } else {
          throw new Error("Failed to load llama.cpp library");
        }
        const smallModelPath = path5.join(modelsDir, config.LOCAL_SMALL_MODEL);
        const largeModelPath = path5.join(modelsDir, config.LOCAL_LARGE_MODEL);
        const embeddingModelPath = path5.join(modelsDir, config.LOCAL_EMBEDDING_MODEL);
        const modelsExist = {
          small: fs5.existsSync(smallModelPath),
          large: fs5.existsSync(largeModelPath),
          embedding: fs5.existsSync(embeddingModelPath)
        };
        if (!modelsExist.small && !modelsExist.large && !modelsExist.embedding) {
          logger8.warn("\u26A0\uFE0F No model files found in models directory");
          logger8.warn("Models will be downloaded on first use, which may take time");
          logger8.warn("To pre-download models, run the plugin and it will fetch them automatically");
        } else {
          logger8.info("\u{1F4E6} Found model files:", {
            small: modelsExist.small ? "\u2713" : "\u2717",
            large: modelsExist.large ? "\u2713" : "\u2717",
            embedding: modelsExist.embedding ? "\u2713" : "\u2717"
          });
        }
        logger8.success("\u2705 Local AI plugin initialized successfully");
        logger8.info("\u{1F4A1} Models will be loaded on-demand when first used");
      } catch (testError) {
        logger8.error("\u274C Local AI initialization test failed:", testError);
        logger8.warn("The plugin may not function correctly");
        logger8.warn("Please check:");
        logger8.warn("1. Your system has sufficient memory (8GB+ recommended)");
        logger8.warn("2. C++ build tools are installed (for node-llama-cpp)");
        logger8.warn("3. Your CPU supports the required instruction sets");
      }
    } catch (error) {
      logger8.error("\u274C Failed to initialize Local AI plugin:", {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : void 0
      });
      if (error instanceof Error) {
        if (error.message.includes("Cannot find module")) {
          logger8.error("\u{1F4DA} Missing dependencies detected");
          logger8.error("Please run: npm install or bun install");
        } else if (error.message.includes("node-llama-cpp")) {
          logger8.error("\u{1F527} node-llama-cpp build issue detected");
          logger8.error("Please ensure C++ build tools are installed:");
          logger8.error("- Windows: Install Visual Studio Build Tools");
          logger8.error("- macOS: Install Xcode Command Line Tools");
          logger8.error("- Linux: Install build-essential package");
        }
      }
      logger8.warn("\u26A0\uFE0F Local AI plugin will not be available");
    }
  },
  models: {
    [ModelType.TEXT_SMALL]: async (runtime, { prompt, stopSequences = [] }) => {
      try {
        await localAIManager.initializeEnvironment();
        return await localAIManager.generateText({
          prompt,
          stopSequences,
          runtime,
          modelType: ModelType.TEXT_SMALL
        });
      } catch (error) {
        logger8.error("Error in TEXT_SMALL handler:", error);
        throw error;
      }
    },
    [ModelType.TEXT_LARGE]: async (runtime, { prompt, stopSequences = [] }) => {
      try {
        await localAIManager.initializeEnvironment();
        return await localAIManager.generateText({
          prompt,
          stopSequences,
          runtime,
          modelType: ModelType.TEXT_LARGE
        });
      } catch (error) {
        logger8.error("Error in TEXT_LARGE handler:", error);
        throw error;
      }
    },
    [ModelType.TEXT_EMBEDDING]: async (_runtime, params) => {
      const text = params?.text;
      try {
        if (!text) {
          logger8.debug("Null or empty text input for embedding, returning zero vector");
          return new Array(384).fill(0);
        }
        return await localAIManager.generateEmbedding(text);
      } catch (error) {
        logger8.error("Error in TEXT_EMBEDDING handler:", {
          error: error instanceof Error ? error.message : String(error),
          fullText: text,
          textType: typeof text,
          textStructure: text !== null ? JSON.stringify(text, null, 2) : "null"
        });
        return new Array(384).fill(0);
      }
    },
    [ModelType.OBJECT_SMALL]: async (runtime, params) => {
      try {
        await localAIManager.initializeEnvironment();
        logger8.info("OBJECT_SMALL handler - Processing request:", {
          prompt: params.prompt,
          hasSchema: !!params.schema,
          temperature: params.temperature
        });
        let jsonPrompt = params.prompt;
        if (!jsonPrompt.includes("```json") && !jsonPrompt.includes("respond with valid JSON")) {
          jsonPrompt += "\nPlease respond with valid JSON only, without any explanations, markdown formatting, or additional text.";
        }
        const textResponse = await localAIManager.generateText({
          prompt: jsonPrompt,
          stopSequences: params.stopSequences,
          runtime,
          modelType: ModelType.TEXT_SMALL
        });
        try {
          const extractJSON = (text) => {
            const jsonBlockRegex = /```(?:json)?\s*([\s\S]*?)\s*```/;
            const match = text.match(jsonBlockRegex);
            if (match && match[1]) {
              return match[1].trim();
            }
            const jsonContentRegex = /\s*(\{[\s\S]*\})\s*$/;
            const contentMatch = text.match(jsonContentRegex);
            if (contentMatch && contentMatch[1]) {
              return contentMatch[1].trim();
            }
            return text.trim();
          };
          const extractedJsonText = extractJSON(textResponse);
          logger8.debug("Extracted JSON text:", extractedJsonText);
          let jsonObject;
          try {
            jsonObject = JSON.parse(extractedJsonText);
          } catch (parseError) {
            logger8.debug("Initial JSON parse failed, attempting to fix common issues");
            const fixedJson = extractedJsonText.replace(/:\s*"([^"]*)(?:\n)([^"]*)"/g, ': "$1\\n$2"').replace(/"([^"]*?)[^a-zA-Z0-9\s\.,;:\-_\(\)"'\[\]{}]([^"]*?)"/g, '"$1$2"').replace(/(\s*)(\w+)(\s*):/g, '$1"$2"$3:').replace(/,(\s*[\]}])/g, "$1");
            try {
              jsonObject = JSON.parse(fixedJson);
            } catch (finalError) {
              logger8.error("Failed to parse JSON after fixing:", finalError);
              throw new Error("Invalid JSON returned from model");
            }
          }
          if (params.schema) {
            try {
              for (const key of Object.keys(params.schema)) {
                if (!(key in jsonObject)) {
                  jsonObject[key] = null;
                }
              }
            } catch (schemaError) {
              logger8.error("Schema validation failed:", schemaError);
            }
          }
          return jsonObject;
        } catch (parseError) {
          logger8.error("Failed to parse JSON:", parseError);
          logger8.error("Raw response:", textResponse);
          throw new Error("Invalid JSON returned from model");
        }
      } catch (error) {
        logger8.error("Error in OBJECT_SMALL handler:", error);
        throw error;
      }
    },
    [ModelType.OBJECT_LARGE]: async (runtime, params) => {
      try {
        await localAIManager.initializeEnvironment();
        logger8.info("OBJECT_LARGE handler - Processing request:", {
          prompt: params.prompt,
          hasSchema: !!params.schema,
          temperature: params.temperature
        });
        let jsonPrompt = params.prompt;
        if (!jsonPrompt.includes("```json") && !jsonPrompt.includes("respond with valid JSON")) {
          jsonPrompt += "\nPlease respond with valid JSON only, without any explanations, markdown formatting, or additional text.";
        }
        const textResponse = await localAIManager.generateText({
          prompt: jsonPrompt,
          stopSequences: params.stopSequences,
          runtime,
          modelType: ModelType.TEXT_LARGE
        });
        try {
          const extractJSON = (text) => {
            const jsonBlockRegex = /```(?:json)?\s*([\s\S]*?)\s*```/;
            const match = text.match(jsonBlockRegex);
            if (match && match[1]) {
              return match[1].trim();
            }
            const jsonContentRegex = /\s*(\{[\s\S]*\})\s*$/;
            const contentMatch = text.match(jsonContentRegex);
            if (contentMatch && contentMatch[1]) {
              return contentMatch[1].trim();
            }
            return text.trim();
          };
          const cleanupJSON = (jsonText) => {
            return jsonText.replace(/\[DEBUG\].*?(\n|$)/g, "\n").replace(/\[LOG\].*?(\n|$)/g, "\n").replace(/console\.log.*?(\n|$)/g, "\n");
          };
          const extractedJsonText = extractJSON(textResponse);
          const cleanedJsonText = cleanupJSON(extractedJsonText);
          logger8.debug("Extracted JSON text:", cleanedJsonText);
          let jsonObject;
          try {
            jsonObject = JSON.parse(cleanedJsonText);
          } catch (parseError) {
            logger8.debug("Initial JSON parse failed, attempting to fix common issues");
            const fixedJson = cleanedJsonText.replace(/:\s*"([^"]*)(?:\n)([^"]*)"/g, ': "$1\\n$2"').replace(/"([^"]*?)[^a-zA-Z0-9\s\.,;:\-_\(\)"'\[\]{}]([^"]*?)"/g, '"$1$2"').replace(/(\s*)(\w+)(\s*):/g, '$1"$2"$3:').replace(/,(\s*[\]}])/g, "$1");
            try {
              jsonObject = JSON.parse(fixedJson);
            } catch (finalError) {
              logger8.error("Failed to parse JSON after fixing:", finalError);
              throw new Error("Invalid JSON returned from model");
            }
          }
          if (params.schema) {
            try {
              for (const key of Object.keys(params.schema)) {
                if (!(key in jsonObject)) {
                  jsonObject[key] = null;
                }
              }
            } catch (schemaError) {
              logger8.error("Schema validation failed:", schemaError);
            }
          }
          return jsonObject;
        } catch (parseError) {
          logger8.error("Failed to parse JSON:", parseError);
          logger8.error("Raw response:", textResponse);
          throw new Error("Invalid JSON returned from model");
        }
      } catch (error) {
        logger8.error("Error in OBJECT_LARGE handler:", error);
        throw error;
      }
    },
    [ModelType.TEXT_TOKENIZER_ENCODE]: async (_runtime, { text }) => {
      try {
        const manager = localAIManager.getTokenizerManager();
        const config = localAIManager.getActiveModelConfig();
        return await manager.encode(text, config);
      } catch (error) {
        logger8.error("Error in TEXT_TOKENIZER_ENCODE handler:", error);
        throw error;
      }
    },
    [ModelType.TEXT_TOKENIZER_DECODE]: async (_runtime, { tokens }) => {
      try {
        const manager = localAIManager.getTokenizerManager();
        const config = localAIManager.getActiveModelConfig();
        return await manager.decode(tokens, config);
      } catch (error) {
        logger8.error("Error in TEXT_TOKENIZER_DECODE handler:", error);
        throw error;
      }
    },
    [ModelType.IMAGE_DESCRIPTION]: async (_runtime, imageUrl) => {
      try {
        logger8.info("Processing image from URL:", imageUrl);
        const response = await fetch(imageUrl);
        if (!response.ok) {
          throw new Error(`Failed to fetch image: ${response.statusText}`);
        }
        const buffer = Buffer.from(await response.arrayBuffer());
        const mimeType = response.headers.get("content-type") || "image/jpeg";
        return await localAIManager.describeImage(buffer, mimeType);
      } catch (error) {
        logger8.error("Error in IMAGE_DESCRIPTION handler:", {
          error: error instanceof Error ? error.message : String(error),
          imageUrl
        });
        throw error;
      }
    },
    [ModelType.TRANSCRIPTION]: async (_runtime, audioBuffer) => {
      try {
        logger8.info("Processing audio transcription:", {
          bufferSize: audioBuffer.length
        });
        return await localAIManager.transcribeAudio(audioBuffer);
      } catch (error) {
        logger8.error("Error in TRANSCRIPTION handler:", {
          error: error instanceof Error ? error.message : String(error),
          bufferSize: audioBuffer.length
        });
        throw error;
      }
    },
    [ModelType.TEXT_TO_SPEECH]: async (_runtime, text) => {
      try {
        return await localAIManager.generateSpeech(text);
      } catch (error) {
        logger8.error("Error in TEXT_TO_SPEECH handler:", {
          error: error instanceof Error ? error.message : String(error),
          textLength: text.length
        });
        throw error;
      }
    }
  },
  tests: [
    {
      name: "local_ai_plugin_tests",
      tests: [
        {
          name: "local_ai_test_initialization",
          fn: async (runtime) => {
            try {
              logger8.info("Starting initialization test");
              const result = await runtime.useModel(ModelType.TEXT_SMALL, {
                prompt: "Debug Mode: Test initialization. Respond with 'Initialization successful' if you can read this.",
                stopSequences: []
              });
              logger8.info("Model response:", result);
              if (!result || typeof result !== "string") {
                throw new Error("Invalid response from model");
              }
              if (!result.includes("successful")) {
                throw new Error("Model response does not indicate success");
              }
              logger8.success("Initialization test completed successfully");
            } catch (error) {
              logger8.error("Initialization test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_text_large",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TEXT_LARGE model test");
              const result = await runtime.useModel(ModelType.TEXT_LARGE, {
                prompt: "Debug Mode: Generate a one-sentence response about artificial intelligence.",
                stopSequences: []
              });
              logger8.info("Large model response:", result);
              if (!result || typeof result !== "string") {
                throw new Error("Invalid response from large model");
              }
              if (result.length < 10) {
                throw new Error("Response too short, possible model failure");
              }
              logger8.success("TEXT_LARGE test completed successfully");
            } catch (error) {
              logger8.error("TEXT_LARGE test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_text_embedding",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TEXT_EMBEDDING test");
              const embedding = await runtime.useModel(ModelType.TEXT_EMBEDDING, {
                text: "This is a test of the text embedding model."
              });
              logger8.info("Embedding generated with dimensions:", embedding.length);
              if (!Array.isArray(embedding)) {
                throw new Error("Embedding is not an array");
              }
              if (embedding.length === 0) {
                throw new Error("Embedding array is empty");
              }
              if (embedding.some((val) => typeof val !== "number")) {
                throw new Error("Embedding contains non-numeric values");
              }
              const nullEmbedding = await runtime.useModel(ModelType.TEXT_EMBEDDING, null);
              if (!Array.isArray(nullEmbedding) || nullEmbedding.some((val) => val !== 0)) {
                throw new Error("Null input did not return zero vector");
              }
              logger8.success("TEXT_EMBEDDING test completed successfully");
            } catch (error) {
              logger8.error("TEXT_EMBEDDING test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_tokenizer_encode",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TEXT_TOKENIZER_ENCODE test");
              const text = "Hello tokenizer test!";
              const tokens = await runtime.useModel(ModelType.TEXT_TOKENIZER_ENCODE, { text });
              logger8.info("Encoded tokens:", { count: tokens.length });
              if (!Array.isArray(tokens)) {
                throw new Error("Tokens output is not an array");
              }
              if (tokens.length === 0) {
                throw new Error("No tokens generated");
              }
              if (tokens.some((token) => !Number.isInteger(token))) {
                throw new Error("Tokens contain non-integer values");
              }
              logger8.success("TEXT_TOKENIZER_ENCODE test completed successfully");
            } catch (error) {
              logger8.error("TEXT_TOKENIZER_ENCODE test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_tokenizer_decode",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TEXT_TOKENIZER_DECODE test");
              const originalText = "Hello tokenizer test!";
              const tokens = await runtime.useModel(ModelType.TEXT_TOKENIZER_ENCODE, {
                text: originalText
              });
              const decodedText = await runtime.useModel(ModelType.TEXT_TOKENIZER_DECODE, {
                tokens
              });
              logger8.info("Round trip tokenization:", {
                original: originalText,
                decoded: decodedText
              });
              if (typeof decodedText !== "string") {
                throw new Error("Decoded output is not a string");
              }
              logger8.success("TEXT_TOKENIZER_DECODE test completed successfully");
            } catch (error) {
              logger8.error("TEXT_TOKENIZER_DECODE test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_image_description",
          fn: async (runtime) => {
            try {
              logger8.info("Starting IMAGE_DESCRIPTION test");
              const imageUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg";
              const result = await runtime.useModel(ModelType.IMAGE_DESCRIPTION, imageUrl);
              logger8.info("Image description result:", result);
              if (!result || typeof result !== "object") {
                throw new Error("Invalid response format");
              }
              if (!result.title || !result.description) {
                throw new Error("Missing title or description in response");
              }
              if (typeof result.title !== "string" || typeof result.description !== "string") {
                throw new Error("Title or description is not a string");
              }
              logger8.success("IMAGE_DESCRIPTION test completed successfully");
            } catch (error) {
              logger8.error("IMAGE_DESCRIPTION test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_transcription",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TRANSCRIPTION test");
              const channels = 1;
              const sampleRate = 16e3;
              const bitsPerSample = 16;
              const duration = 0.5;
              const numSamples = Math.floor(sampleRate * duration);
              const dataSize = numSamples * channels * (bitsPerSample / 8);
              const buffer = Buffer.alloc(44 + dataSize);
              buffer.write("RIFF", 0);
              buffer.writeUInt32LE(36 + dataSize, 4);
              buffer.write("WAVE", 8);
              buffer.write("fmt ", 12);
              buffer.writeUInt32LE(16, 16);
              buffer.writeUInt16LE(1, 20);
              buffer.writeUInt16LE(channels, 22);
              buffer.writeUInt32LE(sampleRate, 24);
              buffer.writeUInt32LE(sampleRate * channels * (bitsPerSample / 8), 28);
              buffer.writeUInt16LE(channels * (bitsPerSample / 8), 32);
              buffer.writeUInt16LE(bitsPerSample, 34);
              buffer.write("data", 36);
              buffer.writeUInt32LE(dataSize, 40);
              const frequency = 440;
              for (let i = 0; i < numSamples; i++) {
                const sample = Math.sin(2 * Math.PI * frequency * i / sampleRate) * 0.1 * 32767;
                buffer.writeInt16LE(Math.floor(sample), 44 + i * 2);
              }
              const transcription = await runtime.useModel(ModelType.TRANSCRIPTION, buffer);
              logger8.info("Transcription result:", transcription);
              if (typeof transcription !== "string") {
                throw new Error("Transcription result is not a string");
              }
              logger8.info("Transcription completed (may be empty for non-speech audio)");
              logger8.success("TRANSCRIPTION test completed successfully");
            } catch (error) {
              logger8.error("TRANSCRIPTION test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        },
        {
          name: "local_ai_test_text_to_speech",
          fn: async (runtime) => {
            try {
              logger8.info("Starting TEXT_TO_SPEECH test");
              const testText = "This is a test of the text to speech system.";
              const audioStream = await runtime.useModel(ModelType.TEXT_TO_SPEECH, testText);
              if (!(audioStream instanceof Readable2)) {
                throw new Error("TTS output is not a readable stream");
              }
              let dataReceived = false;
              audioStream.on("data", () => {
                dataReceived = true;
              });
              await new Promise((resolve, reject) => {
                audioStream.on("end", () => {
                  if (!dataReceived) {
                    reject(new Error("No audio data received from stream"));
                  } else {
                    resolve(true);
                  }
                });
                audioStream.on("error", reject);
              });
              logger8.success("TEXT_TO_SPEECH test completed successfully");
            } catch (error) {
              logger8.error("TEXT_TO_SPEECH test failed:", {
                error: error instanceof Error ? error.message : String(error),
                stack: error instanceof Error ? error.stack : void 0
              });
              throw error;
            }
          }
        }
      ]
    }
  ]
};
var index_default = localAiPlugin;
export {
  index_default as default,
  localAiPlugin
};
//# sourceMappingURL=index.js.map